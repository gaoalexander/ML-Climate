{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "cuda = 'gpu:0' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MAT Data Files\n",
    "\n",
    "matpath_upper = \"/Volumes/RESEARCH1/CAOS/_data/Training/psiTrain/psiTrain1_30km.mat\"\n",
    "matpath_mid = \"/Volumes/RESEARCH1/CAOS/_data/Training/psiTrain/psiTrain1_30km_mid.mat\"\n",
    "\n",
    "psi1_upper = loadmat(matpath_upper)\n",
    "psi1_mid = loadmat(matpath_mid)\n",
    "\n",
    "psi1_upper_30km_filter = psi1_upper['psi1_30km'].astype(np.float64)\n",
    "psi1_upper_anomaly = psi1_upper['psi1Anom'].astype(np.float64)\n",
    "psi1_upper_unfiltered = psi1_upper_30km_filter + psi1_upper_anomaly\n",
    "\n",
    "psi1_mid_unfiltered = psi1_mid['psi1_mid'].astype(np.float64)\n",
    "psi1_top_30km = psi1_mid['psi1_top_30km'].astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class psi1_Dataset:\n",
    "    \"\"\"psiTrain 30km mid Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, X, Y, is_train, train_split):\n",
    "        n_total_samples = len(X[0, 0, :])\n",
    "        split_idx = int(np.floor(train_split * n_total_samples))\n",
    "\n",
    "        x_train = X[:, :, :split_idx]\n",
    "        y_train = Y[:, :, :split_idx]\n",
    "\n",
    "        x_test = X[:, :, split_idx:]\n",
    "        y_test = Y[:, :, split_idx:]\n",
    "\n",
    "        # Differentiate Train v. Test & Normalize\n",
    "        \n",
    "        x_mean, x_std = x_train.mean(), x_train.std()\n",
    "        y_mean, y_std = y_train.mean(), y_train.std()\n",
    "        \n",
    "        if is_train:\n",
    "            self.x = (x_train - x_mean) / x_std\n",
    "            self.y = (y_train - y_mean) / y_std\n",
    "        else:\n",
    "            self.x = (x_test - x_mean) / x_std\n",
    "            self.y = (y_test - y_mean) / y_std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x[0, 0, :])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx): idx = idx.tolist()        \n",
    "        return self.x[:, :, idx], self.y[:, :, idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_split = 0.888888\n",
    "shuffle_flag = True\n",
    "\n",
    "# Create Train/Test datasets\n",
    "\n",
    "train_dataset = psi1_Dataset(X = psi1_upper_30km_filter,\n",
    "                             Y = psi1_upper_anomaly,\n",
    "                             is_train = True,\n",
    "                             train_split = train_split)\n",
    "\n",
    "test_dataset  = psi1_Dataset(X = psi1_upper_30km_filter,\n",
    "                             Y = psi1_upper_anomaly,\n",
    "                             is_train = False,\n",
    "                             train_split = train_split)\n",
    "\n",
    "\n",
    "# Create Data Loaders\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = shuffle_flag)\n",
    "\n",
    "test_loader  = DataLoader(test_dataset,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = shuffle_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        bias_flag = True\n",
    "        \n",
    "        self.conv1 = nn.Conv2d( in_channels = 1,   out_channels = 128, kernel_size = 3, bias = bias_flag, padding = 1 )\n",
    "        self.conv2 = nn.Conv2d( in_channels = 128, out_channels = 64,  kernel_size = 3, bias = bias_flag, padding = 1 )\n",
    "        self.conv3 = nn.Conv2d( in_channels = 64,  out_channels = 48,  kernel_size = 3, bias = bias_flag, padding = 1 )\n",
    "        self.conv4 = nn.Conv2d( in_channels = 48,  out_channels = 1,   kernel_size = 3, bias = bias_flag, padding = 1 )\n",
    "        \n",
    "#         # Experimental - Fixed kernel layer\n",
    "#         f = np.random.rand(5, 5).astype(np.float32)\n",
    "#         f = f.reshape(1, 1, f.shape[0], f.shape[1])\n",
    "#         f = np.repeat(f, 100, axis=1)\n",
    "#         f = np.repeat(f, 100, axis=0)\n",
    "#         self.f = nn.Parameter(data=torch.FloatTensor(f), requires_grad=False)\n",
    "#         self.conv4 = nn.conv2d(x, self.f)\n",
    "    \n",
    "        self.conv1_bn = nn.BatchNorm2d(128)\n",
    "        self.conv2_bn = nn.BatchNorm2d(64)\n",
    "        self.conv3_bn = nn.BatchNorm2d(48)\n",
    "\n",
    "    def forward(self, X, verbose = False):\n",
    "        X = F.selu(self.conv1(X))\n",
    "        X = self.conv1_bn(X)\n",
    "        \n",
    "        X = F.selu(self.conv2(X))\n",
    "        X = self.conv2_bn(X)\n",
    "\n",
    "        X = F.selu(self.conv3(X))\n",
    "        X = self.conv3_bn(X)\n",
    "        \n",
    "        return self.conv4(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_list = []\n",
    "\n",
    "\n",
    "def get_n_params(model):\n",
    "    n = 0\n",
    "    for p in list(model.parameters()):\n",
    "        n += p.nelement()\n",
    "    return n\n",
    "\n",
    "def print_progress(epoch, batch_idx, data, train_loader, loss):\n",
    "    training_loss_list.append(loss.item())\n",
    "    if batch_idx % 1 == 0:\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMSE Loss: {:.6f}'.format(epoch,\n",
    "                                                                           batch_idx * len(data),\n",
    "                                                                           len(train_loader.dataset),\n",
    "                                                                           100. * batch_idx / len(train_loader),\n",
    "                                                                           loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, train_loader):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.float().to(device)\n",
    "        data = data.view(-1, 1, 160, 160)\n",
    "        \n",
    "        target = target.float().to(device)\n",
    "        target = target.view(-1, 1, 160, 160)\n",
    "        \n",
    "        #BACKPROP STEP\n",
    "        output = model(data)\n",
    "        loss = F.mse_loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print_progress(epoch, batch_idx, data, train_loader, loss)\n",
    "            \n",
    "            \n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    n_samples = len(test_loader.dataset)\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data = data.float().to(device)\n",
    "        data = data.view(-1, 1, 160, 160)\n",
    "\n",
    "        target = target.float().to(device) \n",
    "        target = target.view(-1, 1, 160, 160)\n",
    "        \n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output, target).item() # sum up batch loss      \n",
    "\n",
    "    test_loss /= n_samples\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}'.format(test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 103681\n",
      "Train Epoch: 0 [0/3244 (0%)]\tMSE Loss: 1.101552\n",
      "Train Epoch: 0 [64/3244 (2%)]\tMSE Loss: 0.859136\n",
      "Train Epoch: 0 [128/3244 (4%)]\tMSE Loss: 0.824153\n",
      "Train Epoch: 0 [192/3244 (6%)]\tMSE Loss: 0.779865\n",
      "Train Epoch: 0 [256/3244 (8%)]\tMSE Loss: 0.736399\n",
      "Train Epoch: 0 [320/3244 (10%)]\tMSE Loss: 0.716210\n",
      "Train Epoch: 0 [384/3244 (12%)]\tMSE Loss: 0.705513\n",
      "Train Epoch: 0 [448/3244 (14%)]\tMSE Loss: 0.693554\n",
      "Train Epoch: 0 [512/3244 (16%)]\tMSE Loss: 0.672866\n",
      "Train Epoch: 0 [576/3244 (18%)]\tMSE Loss: 0.644211\n",
      "Train Epoch: 0 [640/3244 (20%)]\tMSE Loss: 0.638354\n",
      "Train Epoch: 0 [704/3244 (22%)]\tMSE Loss: 0.629151\n",
      "Train Epoch: 0 [768/3244 (24%)]\tMSE Loss: 0.590493\n",
      "Train Epoch: 0 [832/3244 (25%)]\tMSE Loss: 0.580375\n",
      "Train Epoch: 0 [896/3244 (27%)]\tMSE Loss: 0.576244\n",
      "Train Epoch: 0 [960/3244 (29%)]\tMSE Loss: 0.559963\n",
      "Train Epoch: 0 [1024/3244 (31%)]\tMSE Loss: 0.531471\n",
      "Train Epoch: 0 [1088/3244 (33%)]\tMSE Loss: 0.511189\n",
      "Train Epoch: 0 [1152/3244 (35%)]\tMSE Loss: 0.513271\n",
      "Train Epoch: 0 [1216/3244 (37%)]\tMSE Loss: 0.496003\n",
      "Train Epoch: 0 [1280/3244 (39%)]\tMSE Loss: 0.484915\n",
      "Train Epoch: 0 [1344/3244 (41%)]\tMSE Loss: 0.478787\n",
      "Train Epoch: 0 [1408/3244 (43%)]\tMSE Loss: 0.480126\n",
      "Train Epoch: 0 [1472/3244 (45%)]\tMSE Loss: 0.452568\n",
      "Train Epoch: 0 [1536/3244 (47%)]\tMSE Loss: 0.445269\n",
      "Train Epoch: 0 [1600/3244 (49%)]\tMSE Loss: 0.429763\n",
      "Train Epoch: 0 [1664/3244 (51%)]\tMSE Loss: 0.411404\n",
      "Train Epoch: 0 [1728/3244 (53%)]\tMSE Loss: 0.416049\n",
      "Train Epoch: 0 [1792/3244 (55%)]\tMSE Loss: 0.398695\n",
      "Train Epoch: 0 [1856/3244 (57%)]\tMSE Loss: 0.408713\n",
      "Train Epoch: 0 [1920/3244 (59%)]\tMSE Loss: 0.392639\n",
      "Train Epoch: 0 [1984/3244 (61%)]\tMSE Loss: 0.369478\n",
      "Train Epoch: 0 [2048/3244 (63%)]\tMSE Loss: 0.362507\n",
      "Train Epoch: 0 [2112/3244 (65%)]\tMSE Loss: 0.358513\n",
      "Train Epoch: 0 [2176/3244 (67%)]\tMSE Loss: 0.360681\n",
      "Train Epoch: 0 [2240/3244 (69%)]\tMSE Loss: 0.344818\n",
      "Train Epoch: 0 [2304/3244 (71%)]\tMSE Loss: 0.339148\n",
      "Train Epoch: 0 [2368/3244 (73%)]\tMSE Loss: 0.307226\n",
      "Train Epoch: 0 [2432/3244 (75%)]\tMSE Loss: 0.309888\n",
      "Train Epoch: 0 [2496/3244 (76%)]\tMSE Loss: 0.298787\n",
      "Train Epoch: 0 [2560/3244 (78%)]\tMSE Loss: 0.300517\n",
      "Train Epoch: 0 [2624/3244 (80%)]\tMSE Loss: 0.282040\n",
      "Train Epoch: 0 [2688/3244 (82%)]\tMSE Loss: 0.261689\n",
      "Train Epoch: 0 [2752/3244 (84%)]\tMSE Loss: 0.268870\n",
      "Train Epoch: 0 [2816/3244 (86%)]\tMSE Loss: 0.264843\n",
      "Train Epoch: 0 [2880/3244 (88%)]\tMSE Loss: 0.265716\n",
      "Train Epoch: 0 [2944/3244 (90%)]\tMSE Loss: 0.251454\n",
      "Train Epoch: 0 [3008/3244 (92%)]\tMSE Loss: 0.243888\n",
      "Train Epoch: 0 [3072/3244 (94%)]\tMSE Loss: 0.245148\n",
      "Train Epoch: 0 [3136/3244 (96%)]\tMSE Loss: 0.235570\n",
      "Train Epoch: 0 [2200/3244 (98%)]\tMSE Loss: 0.227216\n",
      "Train Epoch: 1 [0/3244 (0%)]\tMSE Loss: 0.222048\n",
      "Train Epoch: 1 [64/3244 (2%)]\tMSE Loss: 0.215195\n",
      "Train Epoch: 1 [128/3244 (4%)]\tMSE Loss: 0.212272\n",
      "Train Epoch: 1 [192/3244 (6%)]\tMSE Loss: 0.206703\n",
      "Train Epoch: 1 [256/3244 (8%)]\tMSE Loss: 0.204479\n",
      "Train Epoch: 1 [320/3244 (10%)]\tMSE Loss: 0.195667\n",
      "Train Epoch: 1 [384/3244 (12%)]\tMSE Loss: 0.196372\n",
      "Train Epoch: 1 [448/3244 (14%)]\tMSE Loss: 0.196177\n",
      "Train Epoch: 1 [512/3244 (16%)]\tMSE Loss: 0.192163\n",
      "Train Epoch: 1 [576/3244 (18%)]\tMSE Loss: 0.198851\n",
      "Train Epoch: 1 [640/3244 (20%)]\tMSE Loss: 0.184956\n",
      "Train Epoch: 1 [704/3244 (22%)]\tMSE Loss: 0.172975\n",
      "Train Epoch: 1 [768/3244 (24%)]\tMSE Loss: 0.175449\n",
      "Train Epoch: 1 [832/3244 (25%)]\tMSE Loss: 0.172876\n",
      "Train Epoch: 1 [896/3244 (27%)]\tMSE Loss: 0.170268\n",
      "Train Epoch: 1 [960/3244 (29%)]\tMSE Loss: 0.163769\n",
      "Train Epoch: 1 [1024/3244 (31%)]\tMSE Loss: 0.172317\n",
      "Train Epoch: 1 [1088/3244 (33%)]\tMSE Loss: 0.170649\n",
      "Train Epoch: 1 [1152/3244 (35%)]\tMSE Loss: 0.161261\n",
      "Train Epoch: 1 [1216/3244 (37%)]\tMSE Loss: 0.153175\n",
      "Train Epoch: 1 [1280/3244 (39%)]\tMSE Loss: 0.149390\n",
      "Train Epoch: 1 [1344/3244 (41%)]\tMSE Loss: 0.147933\n",
      "Train Epoch: 1 [1408/3244 (43%)]\tMSE Loss: 0.147512\n",
      "Train Epoch: 1 [1472/3244 (45%)]\tMSE Loss: 0.146088\n",
      "Train Epoch: 1 [1536/3244 (47%)]\tMSE Loss: 0.154265\n",
      "Train Epoch: 1 [1600/3244 (49%)]\tMSE Loss: 0.154994\n",
      "Train Epoch: 1 [1664/3244 (51%)]\tMSE Loss: 0.136966\n",
      "Train Epoch: 1 [1728/3244 (53%)]\tMSE Loss: 0.141430\n",
      "Train Epoch: 1 [1792/3244 (55%)]\tMSE Loss: 0.141272\n",
      "Train Epoch: 1 [1856/3244 (57%)]\tMSE Loss: 0.139449\n",
      "Train Epoch: 1 [1920/3244 (59%)]\tMSE Loss: 0.136839\n",
      "Train Epoch: 1 [1984/3244 (61%)]\tMSE Loss: 0.131543\n",
      "Train Epoch: 1 [2048/3244 (63%)]\tMSE Loss: 0.139867\n",
      "Train Epoch: 1 [2112/3244 (65%)]\tMSE Loss: 0.128884\n",
      "Train Epoch: 1 [2176/3244 (67%)]\tMSE Loss: 0.122361\n",
      "Train Epoch: 1 [2240/3244 (69%)]\tMSE Loss: 0.127807\n",
      "Train Epoch: 1 [2304/3244 (71%)]\tMSE Loss: 0.129607\n",
      "Train Epoch: 1 [2368/3244 (73%)]\tMSE Loss: 0.116781\n",
      "Train Epoch: 1 [2432/3244 (75%)]\tMSE Loss: 0.121822\n",
      "Train Epoch: 1 [2496/3244 (76%)]\tMSE Loss: 0.119237\n",
      "Train Epoch: 1 [2560/3244 (78%)]\tMSE Loss: 0.118117\n",
      "Train Epoch: 1 [2624/3244 (80%)]\tMSE Loss: 0.112061\n",
      "Train Epoch: 1 [2688/3244 (82%)]\tMSE Loss: 0.108178\n",
      "Train Epoch: 1 [2752/3244 (84%)]\tMSE Loss: 0.111165\n",
      "Train Epoch: 1 [2816/3244 (86%)]\tMSE Loss: 0.119995\n",
      "Train Epoch: 1 [2880/3244 (88%)]\tMSE Loss: 0.102882\n",
      "Train Epoch: 1 [2944/3244 (90%)]\tMSE Loss: 0.106067\n",
      "Train Epoch: 1 [3008/3244 (92%)]\tMSE Loss: 0.106931\n",
      "Train Epoch: 1 [3072/3244 (94%)]\tMSE Loss: 0.111067\n",
      "Train Epoch: 1 [3136/3244 (96%)]\tMSE Loss: 0.104128\n",
      "Train Epoch: 1 [2200/3244 (98%)]\tMSE Loss: 0.095635\n",
      "Train Epoch: 2 [0/3244 (0%)]\tMSE Loss: 0.103453\n",
      "Train Epoch: 2 [64/3244 (2%)]\tMSE Loss: 0.107905\n",
      "Train Epoch: 2 [128/3244 (4%)]\tMSE Loss: 0.098657\n",
      "Train Epoch: 2 [192/3244 (6%)]\tMSE Loss: 0.095566\n",
      "Train Epoch: 2 [256/3244 (8%)]\tMSE Loss: 0.094862\n",
      "Train Epoch: 2 [320/3244 (10%)]\tMSE Loss: 0.094971\n",
      "Train Epoch: 2 [384/3244 (12%)]\tMSE Loss: 0.097081\n",
      "Train Epoch: 2 [448/3244 (14%)]\tMSE Loss: 0.092328\n",
      "Train Epoch: 2 [512/3244 (16%)]\tMSE Loss: 0.088293\n",
      "Train Epoch: 2 [576/3244 (18%)]\tMSE Loss: 0.084644\n",
      "Train Epoch: 2 [640/3244 (20%)]\tMSE Loss: 0.086796\n",
      "Train Epoch: 2 [704/3244 (22%)]\tMSE Loss: 0.081074\n",
      "Train Epoch: 2 [768/3244 (24%)]\tMSE Loss: 0.085581\n",
      "Train Epoch: 2 [832/3244 (25%)]\tMSE Loss: 0.081612\n",
      "Train Epoch: 2 [896/3244 (27%)]\tMSE Loss: 0.079507\n",
      "Train Epoch: 2 [960/3244 (29%)]\tMSE Loss: 0.075771\n",
      "Train Epoch: 2 [1024/3244 (31%)]\tMSE Loss: 0.077685\n",
      "Train Epoch: 2 [1088/3244 (33%)]\tMSE Loss: 0.077060\n",
      "Train Epoch: 2 [1152/3244 (35%)]\tMSE Loss: 0.079251\n",
      "Train Epoch: 2 [1216/3244 (37%)]\tMSE Loss: 0.076292\n",
      "Train Epoch: 2 [1280/3244 (39%)]\tMSE Loss: 0.085687\n",
      "Train Epoch: 2 [1344/3244 (41%)]\tMSE Loss: 0.092445\n",
      "Train Epoch: 2 [1408/3244 (43%)]\tMSE Loss: 0.098558\n",
      "Train Epoch: 2 [1472/3244 (45%)]\tMSE Loss: 0.077535\n",
      "Train Epoch: 2 [1536/3244 (47%)]\tMSE Loss: 0.078519\n",
      "Train Epoch: 2 [1600/3244 (49%)]\tMSE Loss: 0.088746\n",
      "Train Epoch: 2 [1664/3244 (51%)]\tMSE Loss: 0.073785\n",
      "Train Epoch: 2 [1728/3244 (53%)]\tMSE Loss: 0.070000\n",
      "Train Epoch: 2 [1792/3244 (55%)]\tMSE Loss: 0.075626\n",
      "Train Epoch: 2 [1856/3244 (57%)]\tMSE Loss: 0.073198\n",
      "Train Epoch: 2 [1920/3244 (59%)]\tMSE Loss: 0.070406\n",
      "Train Epoch: 2 [1984/3244 (61%)]\tMSE Loss: 0.076360\n",
      "Train Epoch: 2 [2048/3244 (63%)]\tMSE Loss: 0.074648\n",
      "Train Epoch: 2 [2112/3244 (65%)]\tMSE Loss: 0.070134\n",
      "Train Epoch: 2 [2176/3244 (67%)]\tMSE Loss: 0.068246\n",
      "Train Epoch: 2 [2240/3244 (69%)]\tMSE Loss: 0.069338\n",
      "Train Epoch: 2 [2304/3244 (71%)]\tMSE Loss: 0.066326\n",
      "Train Epoch: 2 [2368/3244 (73%)]\tMSE Loss: 0.066714\n",
      "Train Epoch: 2 [2432/3244 (75%)]\tMSE Loss: 0.067649\n",
      "Train Epoch: 2 [2496/3244 (76%)]\tMSE Loss: 0.066590\n",
      "Train Epoch: 2 [2560/3244 (78%)]\tMSE Loss: 0.064917\n",
      "Train Epoch: 2 [2624/3244 (80%)]\tMSE Loss: 0.060230\n",
      "Train Epoch: 2 [2688/3244 (82%)]\tMSE Loss: 0.067619\n",
      "Train Epoch: 2 [2752/3244 (84%)]\tMSE Loss: 0.061205\n",
      "Train Epoch: 2 [2816/3244 (86%)]\tMSE Loss: 0.061509\n",
      "Train Epoch: 2 [2880/3244 (88%)]\tMSE Loss: 0.059495\n",
      "Train Epoch: 2 [2944/3244 (90%)]\tMSE Loss: 0.063180\n",
      "Train Epoch: 2 [3008/3244 (92%)]\tMSE Loss: 0.058757\n",
      "Train Epoch: 2 [3072/3244 (94%)]\tMSE Loss: 0.059252\n",
      "Train Epoch: 2 [3136/3244 (96%)]\tMSE Loss: 0.060925\n",
      "Train Epoch: 2 [2200/3244 (98%)]\tMSE Loss: 0.060096\n",
      "Train Epoch: 3 [0/3244 (0%)]\tMSE Loss: 0.060726\n",
      "Train Epoch: 3 [64/3244 (2%)]\tMSE Loss: 0.063122\n",
      "Train Epoch: 3 [128/3244 (4%)]\tMSE Loss: 0.059819\n",
      "Train Epoch: 3 [192/3244 (6%)]\tMSE Loss: 0.064279\n",
      "Train Epoch: 3 [256/3244 (8%)]\tMSE Loss: 0.069440\n",
      "Train Epoch: 3 [320/3244 (10%)]\tMSE Loss: 0.072725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [384/3244 (12%)]\tMSE Loss: 0.065660\n",
      "Train Epoch: 3 [448/3244 (14%)]\tMSE Loss: 0.054257\n",
      "Train Epoch: 3 [512/3244 (16%)]\tMSE Loss: 0.073680\n",
      "Train Epoch: 3 [576/3244 (18%)]\tMSE Loss: 0.072542\n",
      "Train Epoch: 3 [640/3244 (20%)]\tMSE Loss: 0.061340\n",
      "Train Epoch: 3 [704/3244 (22%)]\tMSE Loss: 0.071683\n",
      "Train Epoch: 3 [768/3244 (24%)]\tMSE Loss: 0.059819\n",
      "Train Epoch: 3 [832/3244 (25%)]\tMSE Loss: 0.061715\n",
      "Train Epoch: 3 [896/3244 (27%)]\tMSE Loss: 0.063831\n",
      "Train Epoch: 3 [960/3244 (29%)]\tMSE Loss: 0.056502\n",
      "Train Epoch: 3 [1024/3244 (31%)]\tMSE Loss: 0.059596\n",
      "Train Epoch: 3 [1088/3244 (33%)]\tMSE Loss: 0.064194\n",
      "Train Epoch: 3 [1152/3244 (35%)]\tMSE Loss: 0.056459\n",
      "Train Epoch: 3 [1216/3244 (37%)]\tMSE Loss: 0.059374\n",
      "Train Epoch: 3 [1280/3244 (39%)]\tMSE Loss: 0.058134\n",
      "Train Epoch: 3 [1344/3244 (41%)]\tMSE Loss: 0.054742\n",
      "Train Epoch: 3 [1408/3244 (43%)]\tMSE Loss: 0.055892\n",
      "Train Epoch: 3 [1472/3244 (45%)]\tMSE Loss: 0.054586\n",
      "Train Epoch: 3 [1536/3244 (47%)]\tMSE Loss: 0.052997\n",
      "Train Epoch: 3 [1600/3244 (49%)]\tMSE Loss: 0.055497\n",
      "Train Epoch: 3 [1664/3244 (51%)]\tMSE Loss: 0.052322\n",
      "Train Epoch: 3 [1728/3244 (53%)]\tMSE Loss: 0.054964\n",
      "Train Epoch: 3 [1792/3244 (55%)]\tMSE Loss: 0.054969\n",
      "Train Epoch: 3 [1856/3244 (57%)]\tMSE Loss: 0.055957\n",
      "Train Epoch: 3 [1920/3244 (59%)]\tMSE Loss: 0.053359\n",
      "Train Epoch: 3 [1984/3244 (61%)]\tMSE Loss: 0.050554\n",
      "Train Epoch: 3 [2048/3244 (63%)]\tMSE Loss: 0.056337\n",
      "Train Epoch: 3 [2112/3244 (65%)]\tMSE Loss: 0.051284\n",
      "Train Epoch: 3 [2176/3244 (67%)]\tMSE Loss: 0.053243\n",
      "Train Epoch: 3 [2240/3244 (69%)]\tMSE Loss: 0.051493\n",
      "Train Epoch: 3 [2304/3244 (71%)]\tMSE Loss: 0.053712\n",
      "Train Epoch: 3 [2368/3244 (73%)]\tMSE Loss: 0.050355\n",
      "Train Epoch: 3 [2432/3244 (75%)]\tMSE Loss: 0.054363\n",
      "Train Epoch: 3 [2496/3244 (76%)]\tMSE Loss: 0.053462\n",
      "Train Epoch: 3 [2560/3244 (78%)]\tMSE Loss: 0.053741\n",
      "Train Epoch: 3 [2624/3244 (80%)]\tMSE Loss: 0.051439\n",
      "Train Epoch: 3 [2688/3244 (82%)]\tMSE Loss: 0.051955\n",
      "Train Epoch: 3 [2752/3244 (84%)]\tMSE Loss: 0.049234\n",
      "Train Epoch: 3 [2816/3244 (86%)]\tMSE Loss: 0.052360\n",
      "Train Epoch: 3 [2880/3244 (88%)]\tMSE Loss: 0.053143\n",
      "Train Epoch: 3 [2944/3244 (90%)]\tMSE Loss: 0.054486\n",
      "Train Epoch: 3 [3008/3244 (92%)]\tMSE Loss: 0.052361\n",
      "Train Epoch: 3 [3072/3244 (94%)]\tMSE Loss: 0.049657\n",
      "Train Epoch: 3 [3136/3244 (96%)]\tMSE Loss: 0.047831\n",
      "Train Epoch: 3 [2200/3244 (98%)]\tMSE Loss: 0.052223\n",
      "Train Epoch: 4 [0/3244 (0%)]\tMSE Loss: 0.052758\n",
      "Train Epoch: 4 [64/3244 (2%)]\tMSE Loss: 0.049049\n",
      "Train Epoch: 4 [128/3244 (4%)]\tMSE Loss: 0.049614\n",
      "Train Epoch: 4 [192/3244 (6%)]\tMSE Loss: 0.051072\n",
      "Train Epoch: 4 [256/3244 (8%)]\tMSE Loss: 0.049701\n",
      "Train Epoch: 4 [320/3244 (10%)]\tMSE Loss: 0.046905\n",
      "Train Epoch: 4 [384/3244 (12%)]\tMSE Loss: 0.049597\n",
      "Train Epoch: 4 [448/3244 (14%)]\tMSE Loss: 0.048469\n",
      "Train Epoch: 4 [512/3244 (16%)]\tMSE Loss: 0.049428\n",
      "Train Epoch: 4 [576/3244 (18%)]\tMSE Loss: 0.050577\n",
      "Train Epoch: 4 [640/3244 (20%)]\tMSE Loss: 0.051676\n",
      "Train Epoch: 4 [704/3244 (22%)]\tMSE Loss: 0.050683\n",
      "Train Epoch: 4 [768/3244 (24%)]\tMSE Loss: 0.047594\n",
      "Train Epoch: 4 [832/3244 (25%)]\tMSE Loss: 0.051835\n",
      "Train Epoch: 4 [896/3244 (27%)]\tMSE Loss: 0.052587\n",
      "Train Epoch: 4 [960/3244 (29%)]\tMSE Loss: 0.048285\n",
      "Train Epoch: 4 [1024/3244 (31%)]\tMSE Loss: 0.050767\n",
      "Train Epoch: 4 [1088/3244 (33%)]\tMSE Loss: 0.048199\n",
      "Train Epoch: 4 [1152/3244 (35%)]\tMSE Loss: 0.047344\n",
      "Train Epoch: 4 [1216/3244 (37%)]\tMSE Loss: 0.046655\n",
      "Train Epoch: 4 [1280/3244 (39%)]\tMSE Loss: 0.048233\n",
      "Train Epoch: 4 [1344/3244 (41%)]\tMSE Loss: 0.047837\n",
      "Train Epoch: 4 [1408/3244 (43%)]\tMSE Loss: 0.049925\n",
      "Train Epoch: 4 [1472/3244 (45%)]\tMSE Loss: 0.046528\n",
      "Train Epoch: 4 [1536/3244 (47%)]\tMSE Loss: 0.047869\n",
      "Train Epoch: 4 [1600/3244 (49%)]\tMSE Loss: 0.047021\n",
      "Train Epoch: 4 [1664/3244 (51%)]\tMSE Loss: 0.046274\n",
      "Train Epoch: 4 [1728/3244 (53%)]\tMSE Loss: 0.046554\n",
      "Train Epoch: 4 [1792/3244 (55%)]\tMSE Loss: 0.047103\n",
      "Train Epoch: 4 [1856/3244 (57%)]\tMSE Loss: 0.049168\n",
      "Train Epoch: 4 [1920/3244 (59%)]\tMSE Loss: 0.048545\n",
      "Train Epoch: 4 [1984/3244 (61%)]\tMSE Loss: 0.046521\n",
      "Train Epoch: 4 [2048/3244 (63%)]\tMSE Loss: 0.049293\n",
      "Train Epoch: 4 [2112/3244 (65%)]\tMSE Loss: 0.048948\n",
      "Train Epoch: 4 [2176/3244 (67%)]\tMSE Loss: 0.048897\n",
      "Train Epoch: 4 [2240/3244 (69%)]\tMSE Loss: 0.047790\n",
      "Train Epoch: 4 [2304/3244 (71%)]\tMSE Loss: 0.044879\n",
      "Train Epoch: 4 [2368/3244 (73%)]\tMSE Loss: 0.045042\n",
      "Train Epoch: 4 [2432/3244 (75%)]\tMSE Loss: 0.048858\n",
      "Train Epoch: 4 [2496/3244 (76%)]\tMSE Loss: 0.046075\n",
      "Train Epoch: 4 [2560/3244 (78%)]\tMSE Loss: 0.048178\n",
      "Train Epoch: 4 [2624/3244 (80%)]\tMSE Loss: 0.052225\n",
      "Train Epoch: 4 [2688/3244 (82%)]\tMSE Loss: 0.051166\n",
      "Train Epoch: 4 [2752/3244 (84%)]\tMSE Loss: 0.047516\n",
      "Train Epoch: 4 [2816/3244 (86%)]\tMSE Loss: 0.046458\n",
      "Train Epoch: 4 [2880/3244 (88%)]\tMSE Loss: 0.052825\n",
      "Train Epoch: 4 [2944/3244 (90%)]\tMSE Loss: 0.049252\n",
      "Train Epoch: 4 [3008/3244 (92%)]\tMSE Loss: 0.045366\n",
      "Train Epoch: 4 [3072/3244 (94%)]\tMSE Loss: 0.048609\n",
      "Train Epoch: 4 [3136/3244 (96%)]\tMSE Loss: 0.048229\n",
      "Train Epoch: 4 [2200/3244 (98%)]\tMSE Loss: 0.048357\n",
      "Train Epoch: 5 [0/3244 (0%)]\tMSE Loss: 0.048030\n",
      "Train Epoch: 5 [64/3244 (2%)]\tMSE Loss: 0.047782\n",
      "Train Epoch: 5 [128/3244 (4%)]\tMSE Loss: 0.049858\n",
      "Train Epoch: 5 [192/3244 (6%)]\tMSE Loss: 0.047174\n",
      "Train Epoch: 5 [256/3244 (8%)]\tMSE Loss: 0.044125\n",
      "Train Epoch: 5 [320/3244 (10%)]\tMSE Loss: 0.044596\n",
      "Train Epoch: 5 [384/3244 (12%)]\tMSE Loss: 0.048416\n",
      "Train Epoch: 5 [448/3244 (14%)]\tMSE Loss: 0.045848\n",
      "Train Epoch: 5 [512/3244 (16%)]\tMSE Loss: 0.046555\n",
      "Train Epoch: 5 [576/3244 (18%)]\tMSE Loss: 0.046801\n",
      "Train Epoch: 5 [640/3244 (20%)]\tMSE Loss: 0.047171\n",
      "Train Epoch: 5 [704/3244 (22%)]\tMSE Loss: 0.044156\n",
      "Train Epoch: 5 [768/3244 (24%)]\tMSE Loss: 0.046209\n",
      "Train Epoch: 5 [832/3244 (25%)]\tMSE Loss: 0.044683\n",
      "Train Epoch: 5 [896/3244 (27%)]\tMSE Loss: 0.045872\n",
      "Train Epoch: 5 [960/3244 (29%)]\tMSE Loss: 0.044932\n",
      "Train Epoch: 5 [1024/3244 (31%)]\tMSE Loss: 0.043694\n",
      "Train Epoch: 5 [1088/3244 (33%)]\tMSE Loss: 0.045383\n",
      "Train Epoch: 5 [1152/3244 (35%)]\tMSE Loss: 0.045200\n",
      "Train Epoch: 5 [1216/3244 (37%)]\tMSE Loss: 0.043399\n",
      "Train Epoch: 5 [1280/3244 (39%)]\tMSE Loss: 0.042570\n",
      "Train Epoch: 5 [1344/3244 (41%)]\tMSE Loss: 0.043342\n",
      "Train Epoch: 5 [1408/3244 (43%)]\tMSE Loss: 0.044760\n",
      "Train Epoch: 5 [1472/3244 (45%)]\tMSE Loss: 0.045016\n",
      "Train Epoch: 5 [1536/3244 (47%)]\tMSE Loss: 0.045595\n",
      "Train Epoch: 5 [1600/3244 (49%)]\tMSE Loss: 0.047566\n",
      "Train Epoch: 5 [1664/3244 (51%)]\tMSE Loss: 0.044668\n",
      "Train Epoch: 5 [1728/3244 (53%)]\tMSE Loss: 0.045765\n",
      "Train Epoch: 5 [1792/3244 (55%)]\tMSE Loss: 0.045297\n",
      "Train Epoch: 5 [1856/3244 (57%)]\tMSE Loss: 0.045851\n",
      "Train Epoch: 5 [1920/3244 (59%)]\tMSE Loss: 0.044183\n",
      "Train Epoch: 5 [1984/3244 (61%)]\tMSE Loss: 0.042694\n",
      "Train Epoch: 5 [2048/3244 (63%)]\tMSE Loss: 0.046546\n",
      "Train Epoch: 5 [2112/3244 (65%)]\tMSE Loss: 0.046112\n",
      "Train Epoch: 5 [2176/3244 (67%)]\tMSE Loss: 0.044400\n",
      "Train Epoch: 5 [2240/3244 (69%)]\tMSE Loss: 0.044122\n",
      "Train Epoch: 5 [2304/3244 (71%)]\tMSE Loss: 0.045149\n",
      "Train Epoch: 5 [2368/3244 (73%)]\tMSE Loss: 0.040798\n",
      "Train Epoch: 5 [2432/3244 (75%)]\tMSE Loss: 0.046129\n",
      "Train Epoch: 5 [2496/3244 (76%)]\tMSE Loss: 0.045974\n",
      "Train Epoch: 5 [2560/3244 (78%)]\tMSE Loss: 0.048949\n",
      "Train Epoch: 5 [2624/3244 (80%)]\tMSE Loss: 0.049916\n",
      "Train Epoch: 5 [2688/3244 (82%)]\tMSE Loss: 0.049664\n",
      "Train Epoch: 5 [2752/3244 (84%)]\tMSE Loss: 0.043790\n",
      "Train Epoch: 5 [2816/3244 (86%)]\tMSE Loss: 0.044872\n",
      "Train Epoch: 5 [2880/3244 (88%)]\tMSE Loss: 0.044180\n",
      "Train Epoch: 5 [2944/3244 (90%)]\tMSE Loss: 0.045757\n",
      "Train Epoch: 5 [3008/3244 (92%)]\tMSE Loss: 0.043748\n",
      "Train Epoch: 5 [3072/3244 (94%)]\tMSE Loss: 0.045142\n",
      "Train Epoch: 5 [3136/3244 (96%)]\tMSE Loss: 0.044622\n",
      "Train Epoch: 5 [2200/3244 (98%)]\tMSE Loss: 0.045306\n",
      "Train Epoch: 6 [0/3244 (0%)]\tMSE Loss: 0.042727\n",
      "Train Epoch: 6 [64/3244 (2%)]\tMSE Loss: 0.045926\n",
      "Train Epoch: 6 [128/3244 (4%)]\tMSE Loss: 0.046538\n",
      "Train Epoch: 6 [192/3244 (6%)]\tMSE Loss: 0.043455\n",
      "Train Epoch: 6 [256/3244 (8%)]\tMSE Loss: 0.046791\n",
      "Train Epoch: 6 [320/3244 (10%)]\tMSE Loss: 0.045225\n",
      "Train Epoch: 6 [384/3244 (12%)]\tMSE Loss: 0.042897\n",
      "Train Epoch: 6 [448/3244 (14%)]\tMSE Loss: 0.044132\n",
      "Train Epoch: 6 [512/3244 (16%)]\tMSE Loss: 0.046269\n",
      "Train Epoch: 6 [576/3244 (18%)]\tMSE Loss: 0.044939\n",
      "Train Epoch: 6 [640/3244 (20%)]\tMSE Loss: 0.044195\n",
      "Train Epoch: 6 [704/3244 (22%)]\tMSE Loss: 0.043677\n",
      "Train Epoch: 6 [768/3244 (24%)]\tMSE Loss: 0.043569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [832/3244 (25%)]\tMSE Loss: 0.044831\n",
      "Train Epoch: 6 [896/3244 (27%)]\tMSE Loss: 0.045717\n",
      "Train Epoch: 6 [960/3244 (29%)]\tMSE Loss: 0.045591\n",
      "Train Epoch: 6 [1024/3244 (31%)]\tMSE Loss: 0.043624\n",
      "Train Epoch: 6 [1088/3244 (33%)]\tMSE Loss: 0.046498\n",
      "Train Epoch: 6 [1152/3244 (35%)]\tMSE Loss: 0.047341\n",
      "Train Epoch: 6 [1216/3244 (37%)]\tMSE Loss: 0.042270\n",
      "Train Epoch: 6 [1280/3244 (39%)]\tMSE Loss: 0.043658\n",
      "Train Epoch: 6 [1344/3244 (41%)]\tMSE Loss: 0.044777\n",
      "Train Epoch: 6 [1408/3244 (43%)]\tMSE Loss: 0.043599\n",
      "Train Epoch: 6 [1472/3244 (45%)]\tMSE Loss: 0.043212\n",
      "Train Epoch: 6 [1536/3244 (47%)]\tMSE Loss: 0.043674\n",
      "Train Epoch: 6 [1600/3244 (49%)]\tMSE Loss: 0.042550\n",
      "Train Epoch: 6 [1664/3244 (51%)]\tMSE Loss: 0.043061\n",
      "Train Epoch: 6 [1728/3244 (53%)]\tMSE Loss: 0.044000\n",
      "Train Epoch: 6 [1792/3244 (55%)]\tMSE Loss: 0.045616\n",
      "Train Epoch: 6 [1856/3244 (57%)]\tMSE Loss: 0.042232\n",
      "Train Epoch: 6 [1920/3244 (59%)]\tMSE Loss: 0.040600\n",
      "Train Epoch: 6 [1984/3244 (61%)]\tMSE Loss: 0.041081\n",
      "Train Epoch: 6 [2048/3244 (63%)]\tMSE Loss: 0.043326\n",
      "Train Epoch: 6 [2112/3244 (65%)]\tMSE Loss: 0.042261\n",
      "Train Epoch: 6 [2176/3244 (67%)]\tMSE Loss: 0.042775\n",
      "Train Epoch: 6 [2240/3244 (69%)]\tMSE Loss: 0.041723\n",
      "Train Epoch: 6 [2304/3244 (71%)]\tMSE Loss: 0.045102\n",
      "Train Epoch: 6 [2368/3244 (73%)]\tMSE Loss: 0.043379\n",
      "Train Epoch: 6 [2432/3244 (75%)]\tMSE Loss: 0.041522\n",
      "Train Epoch: 6 [2496/3244 (76%)]\tMSE Loss: 0.042956\n",
      "Train Epoch: 6 [2560/3244 (78%)]\tMSE Loss: 0.042370\n",
      "Train Epoch: 6 [2624/3244 (80%)]\tMSE Loss: 0.042305\n",
      "Train Epoch: 6 [2688/3244 (82%)]\tMSE Loss: 0.042250\n",
      "Train Epoch: 6 [2752/3244 (84%)]\tMSE Loss: 0.041704\n",
      "Train Epoch: 6 [2816/3244 (86%)]\tMSE Loss: 0.041690\n",
      "Train Epoch: 6 [2880/3244 (88%)]\tMSE Loss: 0.044193\n",
      "Train Epoch: 6 [2944/3244 (90%)]\tMSE Loss: 0.042759\n",
      "Train Epoch: 6 [3008/3244 (92%)]\tMSE Loss: 0.040692\n",
      "Train Epoch: 6 [3072/3244 (94%)]\tMSE Loss: 0.042408\n",
      "Train Epoch: 6 [3136/3244 (96%)]\tMSE Loss: 0.040844\n",
      "Train Epoch: 6 [2200/3244 (98%)]\tMSE Loss: 0.042655\n",
      "Train Epoch: 7 [0/3244 (0%)]\tMSE Loss: 0.042550\n",
      "Train Epoch: 7 [64/3244 (2%)]\tMSE Loss: 0.041356\n",
      "Train Epoch: 7 [128/3244 (4%)]\tMSE Loss: 0.041265\n",
      "Train Epoch: 7 [192/3244 (6%)]\tMSE Loss: 0.042974\n",
      "Train Epoch: 7 [256/3244 (8%)]\tMSE Loss: 0.042238\n",
      "Train Epoch: 7 [320/3244 (10%)]\tMSE Loss: 0.044554\n",
      "Train Epoch: 7 [384/3244 (12%)]\tMSE Loss: 0.045047\n",
      "Train Epoch: 7 [448/3244 (14%)]\tMSE Loss: 0.046991\n",
      "Train Epoch: 7 [512/3244 (16%)]\tMSE Loss: 0.045873\n",
      "Train Epoch: 7 [576/3244 (18%)]\tMSE Loss: 0.044779\n",
      "Train Epoch: 7 [640/3244 (20%)]\tMSE Loss: 0.041856\n",
      "Train Epoch: 7 [704/3244 (22%)]\tMSE Loss: 0.043120\n",
      "Train Epoch: 7 [768/3244 (24%)]\tMSE Loss: 0.044741\n",
      "Train Epoch: 7 [832/3244 (25%)]\tMSE Loss: 0.041373\n",
      "Train Epoch: 7 [896/3244 (27%)]\tMSE Loss: 0.042207\n",
      "Train Epoch: 7 [960/3244 (29%)]\tMSE Loss: 0.045579\n",
      "Train Epoch: 7 [1024/3244 (31%)]\tMSE Loss: 0.042658\n",
      "Train Epoch: 7 [1088/3244 (33%)]\tMSE Loss: 0.044529\n",
      "Train Epoch: 7 [1152/3244 (35%)]\tMSE Loss: 0.043236\n",
      "Train Epoch: 7 [1216/3244 (37%)]\tMSE Loss: 0.046700\n",
      "Train Epoch: 7 [1280/3244 (39%)]\tMSE Loss: 0.042933\n",
      "Train Epoch: 7 [1344/3244 (41%)]\tMSE Loss: 0.044614\n",
      "Train Epoch: 7 [1408/3244 (43%)]\tMSE Loss: 0.042841\n",
      "Train Epoch: 7 [1472/3244 (45%)]\tMSE Loss: 0.043579\n",
      "Train Epoch: 7 [1536/3244 (47%)]\tMSE Loss: 0.041115\n",
      "Train Epoch: 7 [1600/3244 (49%)]\tMSE Loss: 0.042712\n",
      "Train Epoch: 7 [1664/3244 (51%)]\tMSE Loss: 0.040869\n",
      "Train Epoch: 7 [1728/3244 (53%)]\tMSE Loss: 0.042558\n",
      "Train Epoch: 7 [1792/3244 (55%)]\tMSE Loss: 0.041951\n",
      "Train Epoch: 7 [1856/3244 (57%)]\tMSE Loss: 0.043873\n",
      "Train Epoch: 7 [1920/3244 (59%)]\tMSE Loss: 0.042947\n",
      "Train Epoch: 7 [1984/3244 (61%)]\tMSE Loss: 0.040884\n",
      "Train Epoch: 7 [2048/3244 (63%)]\tMSE Loss: 0.042889\n",
      "Train Epoch: 7 [2112/3244 (65%)]\tMSE Loss: 0.040474\n",
      "Train Epoch: 7 [2176/3244 (67%)]\tMSE Loss: 0.040808\n",
      "Train Epoch: 7 [2240/3244 (69%)]\tMSE Loss: 0.042395\n",
      "Train Epoch: 7 [2304/3244 (71%)]\tMSE Loss: 0.043001\n",
      "Train Epoch: 7 [2368/3244 (73%)]\tMSE Loss: 0.044756\n",
      "Train Epoch: 7 [2432/3244 (75%)]\tMSE Loss: 0.042002\n",
      "Train Epoch: 7 [2496/3244 (76%)]\tMSE Loss: 0.044357\n",
      "Train Epoch: 7 [2560/3244 (78%)]\tMSE Loss: 0.040160\n",
      "Train Epoch: 7 [2624/3244 (80%)]\tMSE Loss: 0.044721\n",
      "Train Epoch: 7 [2688/3244 (82%)]\tMSE Loss: 0.041180\n",
      "Train Epoch: 7 [2752/3244 (84%)]\tMSE Loss: 0.045399\n",
      "Train Epoch: 7 [2816/3244 (86%)]\tMSE Loss: 0.042003\n",
      "Train Epoch: 7 [2880/3244 (88%)]\tMSE Loss: 0.042640\n",
      "Train Epoch: 7 [2944/3244 (90%)]\tMSE Loss: 0.043907\n",
      "Train Epoch: 7 [3008/3244 (92%)]\tMSE Loss: 0.043220\n",
      "Train Epoch: 7 [3072/3244 (94%)]\tMSE Loss: 0.041475\n",
      "Train Epoch: 7 [3136/3244 (96%)]\tMSE Loss: 0.042078\n",
      "Train Epoch: 7 [2200/3244 (98%)]\tMSE Loss: 0.041302\n",
      "Train Epoch: 8 [0/3244 (0%)]\tMSE Loss: 0.042668\n",
      "Train Epoch: 8 [64/3244 (2%)]\tMSE Loss: 0.042153\n",
      "Train Epoch: 8 [128/3244 (4%)]\tMSE Loss: 0.045520\n",
      "Train Epoch: 8 [192/3244 (6%)]\tMSE Loss: 0.045226\n",
      "Train Epoch: 8 [256/3244 (8%)]\tMSE Loss: 0.044128\n",
      "Train Epoch: 8 [320/3244 (10%)]\tMSE Loss: 0.040948\n",
      "Train Epoch: 8 [384/3244 (12%)]\tMSE Loss: 0.038824\n",
      "Train Epoch: 8 [448/3244 (14%)]\tMSE Loss: 0.041687\n",
      "Train Epoch: 8 [512/3244 (16%)]\tMSE Loss: 0.042004\n",
      "Train Epoch: 8 [576/3244 (18%)]\tMSE Loss: 0.040914\n",
      "Train Epoch: 8 [640/3244 (20%)]\tMSE Loss: 0.042704\n",
      "Train Epoch: 8 [704/3244 (22%)]\tMSE Loss: 0.044115\n",
      "Train Epoch: 8 [768/3244 (24%)]\tMSE Loss: 0.043857\n",
      "Train Epoch: 8 [832/3244 (25%)]\tMSE Loss: 0.041427\n",
      "Train Epoch: 8 [896/3244 (27%)]\tMSE Loss: 0.043743\n",
      "Train Epoch: 8 [960/3244 (29%)]\tMSE Loss: 0.042637\n",
      "Train Epoch: 8 [1024/3244 (31%)]\tMSE Loss: 0.042377\n",
      "Train Epoch: 8 [1088/3244 (33%)]\tMSE Loss: 0.039978\n",
      "Train Epoch: 8 [1152/3244 (35%)]\tMSE Loss: 0.041959\n",
      "Train Epoch: 8 [1216/3244 (37%)]\tMSE Loss: 0.042843\n",
      "Train Epoch: 8 [1280/3244 (39%)]\tMSE Loss: 0.041839\n",
      "Train Epoch: 8 [1344/3244 (41%)]\tMSE Loss: 0.039871\n",
      "Train Epoch: 8 [1408/3244 (43%)]\tMSE Loss: 0.041563\n",
      "Train Epoch: 8 [1472/3244 (45%)]\tMSE Loss: 0.042813\n",
      "Train Epoch: 8 [1536/3244 (47%)]\tMSE Loss: 0.039166\n",
      "Train Epoch: 8 [1600/3244 (49%)]\tMSE Loss: 0.039250\n",
      "Train Epoch: 8 [1664/3244 (51%)]\tMSE Loss: 0.039906\n",
      "Train Epoch: 8 [1728/3244 (53%)]\tMSE Loss: 0.042509\n",
      "Train Epoch: 8 [1792/3244 (55%)]\tMSE Loss: 0.043261\n",
      "Train Epoch: 8 [1856/3244 (57%)]\tMSE Loss: 0.041476\n",
      "Train Epoch: 8 [1920/3244 (59%)]\tMSE Loss: 0.042243\n",
      "Train Epoch: 8 [1984/3244 (61%)]\tMSE Loss: 0.041257\n",
      "Train Epoch: 8 [2048/3244 (63%)]\tMSE Loss: 0.042632\n",
      "Train Epoch: 8 [2112/3244 (65%)]\tMSE Loss: 0.043255\n",
      "Train Epoch: 8 [2176/3244 (67%)]\tMSE Loss: 0.044749\n",
      "Train Epoch: 8 [2240/3244 (69%)]\tMSE Loss: 0.049180\n",
      "Train Epoch: 8 [2304/3244 (71%)]\tMSE Loss: 0.044687\n",
      "Train Epoch: 8 [2368/3244 (73%)]\tMSE Loss: 0.041483\n",
      "Train Epoch: 8 [2432/3244 (75%)]\tMSE Loss: 0.043868\n",
      "Train Epoch: 8 [2496/3244 (76%)]\tMSE Loss: 0.043149\n",
      "Train Epoch: 8 [2560/3244 (78%)]\tMSE Loss: 0.043775\n",
      "Train Epoch: 8 [2624/3244 (80%)]\tMSE Loss: 0.040432\n",
      "Train Epoch: 8 [2688/3244 (82%)]\tMSE Loss: 0.042242\n",
      "Train Epoch: 8 [2752/3244 (84%)]\tMSE Loss: 0.041041\n",
      "Train Epoch: 8 [2816/3244 (86%)]\tMSE Loss: 0.040781\n",
      "Train Epoch: 8 [2880/3244 (88%)]\tMSE Loss: 0.039469\n",
      "Train Epoch: 8 [2944/3244 (90%)]\tMSE Loss: 0.039783\n",
      "Train Epoch: 8 [3008/3244 (92%)]\tMSE Loss: 0.040363\n",
      "Train Epoch: 8 [3072/3244 (94%)]\tMSE Loss: 0.040870\n",
      "Train Epoch: 8 [3136/3244 (96%)]\tMSE Loss: 0.040349\n",
      "Train Epoch: 8 [2200/3244 (98%)]\tMSE Loss: 0.041163\n",
      "Train Epoch: 9 [0/3244 (0%)]\tMSE Loss: 0.040569\n",
      "Train Epoch: 9 [64/3244 (2%)]\tMSE Loss: 0.039126\n",
      "Train Epoch: 9 [128/3244 (4%)]\tMSE Loss: 0.041490\n",
      "Train Epoch: 9 [192/3244 (6%)]\tMSE Loss: 0.040597\n",
      "Train Epoch: 9 [256/3244 (8%)]\tMSE Loss: 0.039256\n",
      "Train Epoch: 9 [320/3244 (10%)]\tMSE Loss: 0.038382\n",
      "Train Epoch: 9 [384/3244 (12%)]\tMSE Loss: 0.042278\n",
      "Train Epoch: 9 [448/3244 (14%)]\tMSE Loss: 0.041986\n",
      "Train Epoch: 9 [512/3244 (16%)]\tMSE Loss: 0.041239\n",
      "Train Epoch: 9 [576/3244 (18%)]\tMSE Loss: 0.039122\n",
      "Train Epoch: 9 [640/3244 (20%)]\tMSE Loss: 0.040249\n",
      "Train Epoch: 9 [704/3244 (22%)]\tMSE Loss: 0.040498\n",
      "Train Epoch: 9 [768/3244 (24%)]\tMSE Loss: 0.040447\n",
      "Train Epoch: 9 [832/3244 (25%)]\tMSE Loss: 0.040616\n",
      "Train Epoch: 9 [896/3244 (27%)]\tMSE Loss: 0.042051\n",
      "Train Epoch: 9 [960/3244 (29%)]\tMSE Loss: 0.037297\n",
      "Train Epoch: 9 [1024/3244 (31%)]\tMSE Loss: 0.040356\n",
      "Train Epoch: 9 [1088/3244 (33%)]\tMSE Loss: 0.040692\n",
      "Train Epoch: 9 [1152/3244 (35%)]\tMSE Loss: 0.039513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [1216/3244 (37%)]\tMSE Loss: 0.042834\n",
      "Train Epoch: 9 [1280/3244 (39%)]\tMSE Loss: 0.039448\n",
      "Train Epoch: 9 [1344/3244 (41%)]\tMSE Loss: 0.043640\n",
      "Train Epoch: 9 [1408/3244 (43%)]\tMSE Loss: 0.041228\n",
      "Train Epoch: 9 [1472/3244 (45%)]\tMSE Loss: 0.039130\n",
      "Train Epoch: 9 [1536/3244 (47%)]\tMSE Loss: 0.038422\n",
      "Train Epoch: 9 [1600/3244 (49%)]\tMSE Loss: 0.038850\n",
      "Train Epoch: 9 [1664/3244 (51%)]\tMSE Loss: 0.040348\n",
      "Train Epoch: 9 [1728/3244 (53%)]\tMSE Loss: 0.040754\n",
      "Train Epoch: 9 [1792/3244 (55%)]\tMSE Loss: 0.045398\n",
      "Train Epoch: 9 [1856/3244 (57%)]\tMSE Loss: 0.043211\n",
      "Train Epoch: 9 [1920/3244 (59%)]\tMSE Loss: 0.044357\n",
      "Train Epoch: 9 [1984/3244 (61%)]\tMSE Loss: 0.041570\n",
      "Train Epoch: 9 [2048/3244 (63%)]\tMSE Loss: 0.041688\n",
      "Train Epoch: 9 [2112/3244 (65%)]\tMSE Loss: 0.040156\n",
      "Train Epoch: 9 [2176/3244 (67%)]\tMSE Loss: 0.041724\n",
      "Train Epoch: 9 [2240/3244 (69%)]\tMSE Loss: 0.040935\n",
      "Train Epoch: 9 [2304/3244 (71%)]\tMSE Loss: 0.041413\n",
      "Train Epoch: 9 [2368/3244 (73%)]\tMSE Loss: 0.041172\n",
      "Train Epoch: 9 [2432/3244 (75%)]\tMSE Loss: 0.041926\n",
      "Train Epoch: 9 [2496/3244 (76%)]\tMSE Loss: 0.037820\n",
      "Train Epoch: 9 [2560/3244 (78%)]\tMSE Loss: 0.038989\n",
      "Train Epoch: 9 [2624/3244 (80%)]\tMSE Loss: 0.037983\n",
      "Train Epoch: 9 [2688/3244 (82%)]\tMSE Loss: 0.040941\n",
      "Train Epoch: 9 [2752/3244 (84%)]\tMSE Loss: 0.041573\n",
      "Train Epoch: 9 [2816/3244 (86%)]\tMSE Loss: 0.039390\n",
      "Train Epoch: 9 [2880/3244 (88%)]\tMSE Loss: 0.037966\n",
      "Train Epoch: 9 [2944/3244 (90%)]\tMSE Loss: 0.041456\n",
      "Train Epoch: 9 [3008/3244 (92%)]\tMSE Loss: 0.041021\n",
      "Train Epoch: 9 [3072/3244 (94%)]\tMSE Loss: 0.039885\n",
      "Train Epoch: 9 [3136/3244 (96%)]\tMSE Loss: 0.038345\n",
      "Train Epoch: 9 [2200/3244 (98%)]\tMSE Loss: 0.039575\n",
      "Train Epoch: 10 [0/3244 (0%)]\tMSE Loss: 0.040395\n",
      "Train Epoch: 10 [64/3244 (2%)]\tMSE Loss: 0.037035\n",
      "Train Epoch: 10 [128/3244 (4%)]\tMSE Loss: 0.042295\n",
      "Train Epoch: 10 [192/3244 (6%)]\tMSE Loss: 0.038788\n",
      "Train Epoch: 10 [256/3244 (8%)]\tMSE Loss: 0.041105\n",
      "Train Epoch: 10 [320/3244 (10%)]\tMSE Loss: 0.037542\n",
      "Train Epoch: 10 [384/3244 (12%)]\tMSE Loss: 0.041549\n",
      "Train Epoch: 10 [448/3244 (14%)]\tMSE Loss: 0.040348\n",
      "Train Epoch: 10 [512/3244 (16%)]\tMSE Loss: 0.040389\n",
      "Train Epoch: 10 [576/3244 (18%)]\tMSE Loss: 0.041960\n",
      "Train Epoch: 10 [640/3244 (20%)]\tMSE Loss: 0.043537\n",
      "Train Epoch: 10 [704/3244 (22%)]\tMSE Loss: 0.043201\n",
      "Train Epoch: 10 [768/3244 (24%)]\tMSE Loss: 0.042321\n",
      "Train Epoch: 10 [832/3244 (25%)]\tMSE Loss: 0.039161\n",
      "Train Epoch: 10 [896/3244 (27%)]\tMSE Loss: 0.041011\n",
      "Train Epoch: 10 [960/3244 (29%)]\tMSE Loss: 0.045551\n",
      "Train Epoch: 10 [1024/3244 (31%)]\tMSE Loss: 0.048520\n",
      "Train Epoch: 10 [1088/3244 (33%)]\tMSE Loss: 0.041545\n",
      "Train Epoch: 10 [1152/3244 (35%)]\tMSE Loss: 0.040515\n",
      "Train Epoch: 10 [1216/3244 (37%)]\tMSE Loss: 0.044492\n",
      "Train Epoch: 10 [1280/3244 (39%)]\tMSE Loss: 0.042131\n",
      "Train Epoch: 10 [1344/3244 (41%)]\tMSE Loss: 0.038656\n",
      "Train Epoch: 10 [1408/3244 (43%)]\tMSE Loss: 0.039913\n",
      "Train Epoch: 10 [1472/3244 (45%)]\tMSE Loss: 0.041678\n",
      "Train Epoch: 10 [1536/3244 (47%)]\tMSE Loss: 0.041202\n",
      "Train Epoch: 10 [1600/3244 (49%)]\tMSE Loss: 0.041639\n",
      "Train Epoch: 10 [1664/3244 (51%)]\tMSE Loss: 0.040783\n",
      "Train Epoch: 10 [1728/3244 (53%)]\tMSE Loss: 0.040645\n",
      "Train Epoch: 10 [1792/3244 (55%)]\tMSE Loss: 0.041135\n",
      "Train Epoch: 10 [1856/3244 (57%)]\tMSE Loss: 0.042635\n",
      "Train Epoch: 10 [1920/3244 (59%)]\tMSE Loss: 0.044635\n",
      "Train Epoch: 10 [1984/3244 (61%)]\tMSE Loss: 0.040595\n",
      "Train Epoch: 10 [2048/3244 (63%)]\tMSE Loss: 0.038594\n",
      "Train Epoch: 10 [2112/3244 (65%)]\tMSE Loss: 0.041208\n",
      "Train Epoch: 10 [2176/3244 (67%)]\tMSE Loss: 0.038639\n",
      "Train Epoch: 10 [2240/3244 (69%)]\tMSE Loss: 0.038713\n",
      "Train Epoch: 10 [2304/3244 (71%)]\tMSE Loss: 0.038726\n",
      "Train Epoch: 10 [2368/3244 (73%)]\tMSE Loss: 0.040550\n",
      "Train Epoch: 10 [2432/3244 (75%)]\tMSE Loss: 0.036631\n",
      "Train Epoch: 10 [2496/3244 (76%)]\tMSE Loss: 0.037166\n",
      "Train Epoch: 10 [2560/3244 (78%)]\tMSE Loss: 0.038642\n",
      "Train Epoch: 10 [2624/3244 (80%)]\tMSE Loss: 0.038972\n",
      "Train Epoch: 10 [2688/3244 (82%)]\tMSE Loss: 0.038117\n",
      "Train Epoch: 10 [2752/3244 (84%)]\tMSE Loss: 0.041250\n",
      "Train Epoch: 10 [2816/3244 (86%)]\tMSE Loss: 0.038709\n",
      "Train Epoch: 10 [2880/3244 (88%)]\tMSE Loss: 0.040105\n",
      "Train Epoch: 10 [2944/3244 (90%)]\tMSE Loss: 0.040752\n",
      "Train Epoch: 10 [3008/3244 (92%)]\tMSE Loss: 0.042434\n",
      "Train Epoch: 10 [3072/3244 (94%)]\tMSE Loss: 0.042569\n",
      "Train Epoch: 10 [3136/3244 (96%)]\tMSE Loss: 0.038939\n",
      "Train Epoch: 10 [2200/3244 (98%)]\tMSE Loss: 0.039371\n",
      "Train Epoch: 11 [0/3244 (0%)]\tMSE Loss: 0.045641\n",
      "Train Epoch: 11 [64/3244 (2%)]\tMSE Loss: 0.041177\n",
      "Train Epoch: 11 [128/3244 (4%)]\tMSE Loss: 0.037531\n",
      "Train Epoch: 11 [192/3244 (6%)]\tMSE Loss: 0.037595\n",
      "Train Epoch: 11 [256/3244 (8%)]\tMSE Loss: 0.041570\n",
      "Train Epoch: 11 [320/3244 (10%)]\tMSE Loss: 0.040662\n",
      "Train Epoch: 11 [384/3244 (12%)]\tMSE Loss: 0.039146\n",
      "Train Epoch: 11 [448/3244 (14%)]\tMSE Loss: 0.039993\n",
      "Train Epoch: 11 [512/3244 (16%)]\tMSE Loss: 0.038487\n",
      "Train Epoch: 11 [576/3244 (18%)]\tMSE Loss: 0.039858\n",
      "Train Epoch: 11 [640/3244 (20%)]\tMSE Loss: 0.038895\n",
      "Train Epoch: 11 [704/3244 (22%)]\tMSE Loss: 0.037547\n",
      "Train Epoch: 11 [768/3244 (24%)]\tMSE Loss: 0.037605\n",
      "Train Epoch: 11 [832/3244 (25%)]\tMSE Loss: 0.039647\n",
      "Train Epoch: 11 [896/3244 (27%)]\tMSE Loss: 0.039583\n",
      "Train Epoch: 11 [960/3244 (29%)]\tMSE Loss: 0.038738\n",
      "Train Epoch: 11 [1024/3244 (31%)]\tMSE Loss: 0.038072\n",
      "Train Epoch: 11 [1088/3244 (33%)]\tMSE Loss: 0.040892\n",
      "Train Epoch: 11 [1152/3244 (35%)]\tMSE Loss: 0.038483\n",
      "Train Epoch: 11 [1216/3244 (37%)]\tMSE Loss: 0.038220\n",
      "Train Epoch: 11 [1280/3244 (39%)]\tMSE Loss: 0.040502\n",
      "Train Epoch: 11 [1344/3244 (41%)]\tMSE Loss: 0.035741\n",
      "Train Epoch: 11 [1408/3244 (43%)]\tMSE Loss: 0.038463\n",
      "Train Epoch: 11 [1472/3244 (45%)]\tMSE Loss: 0.039336\n",
      "Train Epoch: 11 [1536/3244 (47%)]\tMSE Loss: 0.037397\n",
      "Train Epoch: 11 [1600/3244 (49%)]\tMSE Loss: 0.038131\n",
      "Train Epoch: 11 [1664/3244 (51%)]\tMSE Loss: 0.038331\n",
      "Train Epoch: 11 [1728/3244 (53%)]\tMSE Loss: 0.039922\n",
      "Train Epoch: 11 [1792/3244 (55%)]\tMSE Loss: 0.038350\n",
      "Train Epoch: 11 [1856/3244 (57%)]\tMSE Loss: 0.040437\n",
      "Train Epoch: 11 [1920/3244 (59%)]\tMSE Loss: 0.042051\n",
      "Train Epoch: 11 [1984/3244 (61%)]\tMSE Loss: 0.039915\n",
      "Train Epoch: 11 [2048/3244 (63%)]\tMSE Loss: 0.040556\n",
      "Train Epoch: 11 [2112/3244 (65%)]\tMSE Loss: 0.038160\n",
      "Train Epoch: 11 [2176/3244 (67%)]\tMSE Loss: 0.040477\n",
      "Train Epoch: 11 [2240/3244 (69%)]\tMSE Loss: 0.039179\n",
      "Train Epoch: 11 [2304/3244 (71%)]\tMSE Loss: 0.040940\n",
      "Train Epoch: 11 [2368/3244 (73%)]\tMSE Loss: 0.040797\n",
      "Train Epoch: 11 [2432/3244 (75%)]\tMSE Loss: 0.038750\n",
      "Train Epoch: 11 [2496/3244 (76%)]\tMSE Loss: 0.040162\n",
      "Train Epoch: 11 [2560/3244 (78%)]\tMSE Loss: 0.039373\n",
      "Train Epoch: 11 [2624/3244 (80%)]\tMSE Loss: 0.036711\n",
      "Train Epoch: 11 [2688/3244 (82%)]\tMSE Loss: 0.038288\n",
      "Train Epoch: 11 [2752/3244 (84%)]\tMSE Loss: 0.039257\n",
      "Train Epoch: 11 [2816/3244 (86%)]\tMSE Loss: 0.038785\n",
      "Train Epoch: 11 [2880/3244 (88%)]\tMSE Loss: 0.038951\n",
      "Train Epoch: 11 [2944/3244 (90%)]\tMSE Loss: 0.040177\n",
      "Train Epoch: 11 [3008/3244 (92%)]\tMSE Loss: 0.038583\n",
      "Train Epoch: 11 [3072/3244 (94%)]\tMSE Loss: 0.038328\n",
      "Train Epoch: 11 [3136/3244 (96%)]\tMSE Loss: 0.035495\n",
      "Train Epoch: 11 [2200/3244 (98%)]\tMSE Loss: 0.039189\n",
      "Train Epoch: 12 [0/3244 (0%)]\tMSE Loss: 0.037010\n",
      "Train Epoch: 12 [64/3244 (2%)]\tMSE Loss: 0.038470\n",
      "Train Epoch: 12 [128/3244 (4%)]\tMSE Loss: 0.037609\n",
      "Train Epoch: 12 [192/3244 (6%)]\tMSE Loss: 0.038413\n",
      "Train Epoch: 12 [256/3244 (8%)]\tMSE Loss: 0.037254\n",
      "Train Epoch: 12 [320/3244 (10%)]\tMSE Loss: 0.041544\n",
      "Train Epoch: 12 [384/3244 (12%)]\tMSE Loss: 0.037568\n",
      "Train Epoch: 12 [448/3244 (14%)]\tMSE Loss: 0.041315\n",
      "Train Epoch: 12 [512/3244 (16%)]\tMSE Loss: 0.037919\n",
      "Train Epoch: 12 [576/3244 (18%)]\tMSE Loss: 0.038844\n",
      "Train Epoch: 12 [640/3244 (20%)]\tMSE Loss: 0.038705\n",
      "Train Epoch: 12 [704/3244 (22%)]\tMSE Loss: 0.037795\n",
      "Train Epoch: 12 [768/3244 (24%)]\tMSE Loss: 0.041403\n",
      "Train Epoch: 12 [832/3244 (25%)]\tMSE Loss: 0.040689\n",
      "Train Epoch: 12 [896/3244 (27%)]\tMSE Loss: 0.042378\n",
      "Train Epoch: 12 [960/3244 (29%)]\tMSE Loss: 0.038828\n",
      "Train Epoch: 12 [1024/3244 (31%)]\tMSE Loss: 0.039012\n",
      "Train Epoch: 12 [1088/3244 (33%)]\tMSE Loss: 0.038213\n",
      "Train Epoch: 12 [1152/3244 (35%)]\tMSE Loss: 0.038867\n",
      "Train Epoch: 12 [1216/3244 (37%)]\tMSE Loss: 0.040609\n",
      "Train Epoch: 12 [1280/3244 (39%)]\tMSE Loss: 0.036898\n",
      "Train Epoch: 12 [1344/3244 (41%)]\tMSE Loss: 0.037299\n",
      "Train Epoch: 12 [1408/3244 (43%)]\tMSE Loss: 0.038278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [1472/3244 (45%)]\tMSE Loss: 0.040707\n",
      "Train Epoch: 12 [1536/3244 (47%)]\tMSE Loss: 0.039176\n",
      "Train Epoch: 12 [1600/3244 (49%)]\tMSE Loss: 0.038201\n",
      "Train Epoch: 12 [1664/3244 (51%)]\tMSE Loss: 0.035421\n",
      "Train Epoch: 12 [1728/3244 (53%)]\tMSE Loss: 0.036319\n",
      "Train Epoch: 12 [1792/3244 (55%)]\tMSE Loss: 0.036667\n",
      "Train Epoch: 12 [1856/3244 (57%)]\tMSE Loss: 0.038716\n",
      "Train Epoch: 12 [1920/3244 (59%)]\tMSE Loss: 0.034684\n",
      "Train Epoch: 12 [1984/3244 (61%)]\tMSE Loss: 0.039153\n",
      "Train Epoch: 12 [2048/3244 (63%)]\tMSE Loss: 0.039983\n",
      "Train Epoch: 12 [2112/3244 (65%)]\tMSE Loss: 0.037267\n",
      "Train Epoch: 12 [2176/3244 (67%)]\tMSE Loss: 0.039516\n",
      "Train Epoch: 12 [2240/3244 (69%)]\tMSE Loss: 0.039100\n",
      "Train Epoch: 12 [2304/3244 (71%)]\tMSE Loss: 0.036960\n",
      "Train Epoch: 12 [2368/3244 (73%)]\tMSE Loss: 0.038707\n",
      "Train Epoch: 12 [2432/3244 (75%)]\tMSE Loss: 0.035847\n",
      "Train Epoch: 12 [2496/3244 (76%)]\tMSE Loss: 0.037708\n",
      "Train Epoch: 12 [2560/3244 (78%)]\tMSE Loss: 0.037729\n",
      "Train Epoch: 12 [2624/3244 (80%)]\tMSE Loss: 0.037852\n",
      "Train Epoch: 12 [2688/3244 (82%)]\tMSE Loss: 0.038245\n",
      "Train Epoch: 12 [2752/3244 (84%)]\tMSE Loss: 0.038723\n",
      "Train Epoch: 12 [2816/3244 (86%)]\tMSE Loss: 0.036149\n",
      "Train Epoch: 12 [2880/3244 (88%)]\tMSE Loss: 0.038728\n",
      "Train Epoch: 12 [2944/3244 (90%)]\tMSE Loss: 0.041065\n",
      "Train Epoch: 12 [3008/3244 (92%)]\tMSE Loss: 0.039218\n",
      "Train Epoch: 12 [3072/3244 (94%)]\tMSE Loss: 0.039668\n",
      "Train Epoch: 12 [3136/3244 (96%)]\tMSE Loss: 0.039127\n",
      "Train Epoch: 12 [2200/3244 (98%)]\tMSE Loss: 0.038001\n",
      "Train Epoch: 13 [0/3244 (0%)]\tMSE Loss: 0.040076\n",
      "Train Epoch: 13 [64/3244 (2%)]\tMSE Loss: 0.039656\n",
      "Train Epoch: 13 [128/3244 (4%)]\tMSE Loss: 0.037244\n",
      "Train Epoch: 13 [192/3244 (6%)]\tMSE Loss: 0.035677\n",
      "Train Epoch: 13 [256/3244 (8%)]\tMSE Loss: 0.038206\n",
      "Train Epoch: 13 [320/3244 (10%)]\tMSE Loss: 0.038979\n",
      "Train Epoch: 13 [384/3244 (12%)]\tMSE Loss: 0.036696\n",
      "Train Epoch: 13 [448/3244 (14%)]\tMSE Loss: 0.036374\n",
      "Train Epoch: 13 [512/3244 (16%)]\tMSE Loss: 0.037881\n",
      "Train Epoch: 13 [576/3244 (18%)]\tMSE Loss: 0.037311\n",
      "Train Epoch: 13 [640/3244 (20%)]\tMSE Loss: 0.040915\n",
      "Train Epoch: 13 [704/3244 (22%)]\tMSE Loss: 0.042699\n",
      "Train Epoch: 13 [768/3244 (24%)]\tMSE Loss: 0.038275\n",
      "Train Epoch: 13 [832/3244 (25%)]\tMSE Loss: 0.040207\n",
      "Train Epoch: 13 [896/3244 (27%)]\tMSE Loss: 0.037698\n",
      "Train Epoch: 13 [960/3244 (29%)]\tMSE Loss: 0.040584\n",
      "Train Epoch: 13 [1024/3244 (31%)]\tMSE Loss: 0.038738\n",
      "Train Epoch: 13 [1088/3244 (33%)]\tMSE Loss: 0.042850\n",
      "Train Epoch: 13 [1152/3244 (35%)]\tMSE Loss: 0.039583\n",
      "Train Epoch: 13 [1216/3244 (37%)]\tMSE Loss: 0.039454\n",
      "Train Epoch: 13 [1280/3244 (39%)]\tMSE Loss: 0.039524\n",
      "Train Epoch: 13 [1344/3244 (41%)]\tMSE Loss: 0.038117\n",
      "Train Epoch: 13 [1408/3244 (43%)]\tMSE Loss: 0.037596\n",
      "Train Epoch: 13 [1472/3244 (45%)]\tMSE Loss: 0.038463\n",
      "Train Epoch: 13 [1536/3244 (47%)]\tMSE Loss: 0.038345\n",
      "Train Epoch: 13 [1600/3244 (49%)]\tMSE Loss: 0.037500\n",
      "Train Epoch: 13 [1664/3244 (51%)]\tMSE Loss: 0.036434\n",
      "Train Epoch: 13 [1728/3244 (53%)]\tMSE Loss: 0.038666\n",
      "Train Epoch: 13 [1792/3244 (55%)]\tMSE Loss: 0.038964\n",
      "Train Epoch: 13 [1856/3244 (57%)]\tMSE Loss: 0.037976\n",
      "Train Epoch: 13 [1920/3244 (59%)]\tMSE Loss: 0.038234\n",
      "Train Epoch: 13 [1984/3244 (61%)]\tMSE Loss: 0.039380\n",
      "Train Epoch: 13 [2048/3244 (63%)]\tMSE Loss: 0.038904\n",
      "Train Epoch: 13 [2112/3244 (65%)]\tMSE Loss: 0.038895\n",
      "Train Epoch: 13 [2176/3244 (67%)]\tMSE Loss: 0.037448\n",
      "Train Epoch: 13 [2240/3244 (69%)]\tMSE Loss: 0.037096\n",
      "Train Epoch: 13 [2304/3244 (71%)]\tMSE Loss: 0.037054\n",
      "Train Epoch: 13 [2368/3244 (73%)]\tMSE Loss: 0.040050\n",
      "Train Epoch: 13 [2432/3244 (75%)]\tMSE Loss: 0.039495\n",
      "Train Epoch: 13 [2496/3244 (76%)]\tMSE Loss: 0.037122\n",
      "Train Epoch: 13 [2560/3244 (78%)]\tMSE Loss: 0.040196\n",
      "Train Epoch: 13 [2624/3244 (80%)]\tMSE Loss: 0.038339\n",
      "Train Epoch: 13 [2688/3244 (82%)]\tMSE Loss: 0.040646\n",
      "Train Epoch: 13 [2752/3244 (84%)]\tMSE Loss: 0.036409\n",
      "Train Epoch: 13 [2816/3244 (86%)]\tMSE Loss: 0.040876\n",
      "Train Epoch: 13 [2880/3244 (88%)]\tMSE Loss: 0.037617\n",
      "Train Epoch: 13 [2944/3244 (90%)]\tMSE Loss: 0.036911\n",
      "Train Epoch: 13 [3008/3244 (92%)]\tMSE Loss: 0.036890\n",
      "Train Epoch: 13 [3072/3244 (94%)]\tMSE Loss: 0.036219\n",
      "Train Epoch: 13 [3136/3244 (96%)]\tMSE Loss: 0.038680\n",
      "Train Epoch: 13 [2200/3244 (98%)]\tMSE Loss: 0.039892\n",
      "Train Epoch: 14 [0/3244 (0%)]\tMSE Loss: 0.040741\n",
      "Train Epoch: 14 [64/3244 (2%)]\tMSE Loss: 0.039549\n",
      "Train Epoch: 14 [128/3244 (4%)]\tMSE Loss: 0.043322\n",
      "Train Epoch: 14 [192/3244 (6%)]\tMSE Loss: 0.040243\n",
      "Train Epoch: 14 [256/3244 (8%)]\tMSE Loss: 0.038813\n",
      "Train Epoch: 14 [320/3244 (10%)]\tMSE Loss: 0.038781\n",
      "Train Epoch: 14 [384/3244 (12%)]\tMSE Loss: 0.039422\n",
      "Train Epoch: 14 [448/3244 (14%)]\tMSE Loss: 0.038462\n",
      "Train Epoch: 14 [512/3244 (16%)]\tMSE Loss: 0.037461\n",
      "Train Epoch: 14 [576/3244 (18%)]\tMSE Loss: 0.040753\n",
      "Train Epoch: 14 [640/3244 (20%)]\tMSE Loss: 0.037079\n",
      "Train Epoch: 14 [704/3244 (22%)]\tMSE Loss: 0.037892\n",
      "Train Epoch: 14 [768/3244 (24%)]\tMSE Loss: 0.037760\n",
      "Train Epoch: 14 [832/3244 (25%)]\tMSE Loss: 0.036720\n",
      "Train Epoch: 14 [896/3244 (27%)]\tMSE Loss: 0.036849\n",
      "Train Epoch: 14 [960/3244 (29%)]\tMSE Loss: 0.037912\n",
      "Train Epoch: 14 [1024/3244 (31%)]\tMSE Loss: 0.039551\n",
      "Train Epoch: 14 [1088/3244 (33%)]\tMSE Loss: 0.037556\n",
      "Train Epoch: 14 [1152/3244 (35%)]\tMSE Loss: 0.037395\n",
      "Train Epoch: 14 [1216/3244 (37%)]\tMSE Loss: 0.034173\n",
      "Train Epoch: 14 [1280/3244 (39%)]\tMSE Loss: 0.037032\n",
      "Train Epoch: 14 [1344/3244 (41%)]\tMSE Loss: 0.038387\n",
      "Train Epoch: 14 [1408/3244 (43%)]\tMSE Loss: 0.037068\n",
      "Train Epoch: 14 [1472/3244 (45%)]\tMSE Loss: 0.036092\n",
      "Train Epoch: 14 [1536/3244 (47%)]\tMSE Loss: 0.038230\n",
      "Train Epoch: 14 [1600/3244 (49%)]\tMSE Loss: 0.037976\n",
      "Train Epoch: 14 [1664/3244 (51%)]\tMSE Loss: 0.039194\n",
      "Train Epoch: 14 [1728/3244 (53%)]\tMSE Loss: 0.038245\n",
      "Train Epoch: 14 [1792/3244 (55%)]\tMSE Loss: 0.036416\n",
      "Train Epoch: 14 [1856/3244 (57%)]\tMSE Loss: 0.037162\n",
      "Train Epoch: 14 [1920/3244 (59%)]\tMSE Loss: 0.037728\n",
      "Train Epoch: 14 [1984/3244 (61%)]\tMSE Loss: 0.037845\n",
      "Train Epoch: 14 [2048/3244 (63%)]\tMSE Loss: 0.037851\n",
      "Train Epoch: 14 [2112/3244 (65%)]\tMSE Loss: 0.036403\n",
      "Train Epoch: 14 [2176/3244 (67%)]\tMSE Loss: 0.035555\n",
      "Train Epoch: 14 [2240/3244 (69%)]\tMSE Loss: 0.036723\n",
      "Train Epoch: 14 [2304/3244 (71%)]\tMSE Loss: 0.036886\n",
      "Train Epoch: 14 [2368/3244 (73%)]\tMSE Loss: 0.036142\n",
      "Train Epoch: 14 [2432/3244 (75%)]\tMSE Loss: 0.038068\n",
      "Train Epoch: 14 [2496/3244 (76%)]\tMSE Loss: 0.036388\n",
      "Train Epoch: 14 [2560/3244 (78%)]\tMSE Loss: 0.038453\n",
      "Train Epoch: 14 [2624/3244 (80%)]\tMSE Loss: 0.035877\n",
      "Train Epoch: 14 [2688/3244 (82%)]\tMSE Loss: 0.037931\n",
      "Train Epoch: 14 [2752/3244 (84%)]\tMSE Loss: 0.041489\n",
      "Train Epoch: 14 [2816/3244 (86%)]\tMSE Loss: 0.040687\n",
      "Train Epoch: 14 [2880/3244 (88%)]\tMSE Loss: 0.037772\n",
      "Train Epoch: 14 [2944/3244 (90%)]\tMSE Loss: 0.035544\n",
      "Train Epoch: 14 [3008/3244 (92%)]\tMSE Loss: 0.037486\n",
      "Train Epoch: 14 [3072/3244 (94%)]\tMSE Loss: 0.042160\n",
      "Train Epoch: 14 [3136/3244 (96%)]\tMSE Loss: 0.036687\n",
      "Train Epoch: 14 [2200/3244 (98%)]\tMSE Loss: 0.042567\n",
      "Train Epoch: 15 [0/3244 (0%)]\tMSE Loss: 0.042366\n",
      "Train Epoch: 15 [64/3244 (2%)]\tMSE Loss: 0.040901\n",
      "Train Epoch: 15 [128/3244 (4%)]\tMSE Loss: 0.039204\n",
      "Train Epoch: 15 [192/3244 (6%)]\tMSE Loss: 0.037978\n",
      "Train Epoch: 15 [256/3244 (8%)]\tMSE Loss: 0.040198\n",
      "Train Epoch: 15 [320/3244 (10%)]\tMSE Loss: 0.038654\n",
      "Train Epoch: 15 [384/3244 (12%)]\tMSE Loss: 0.039323\n",
      "Train Epoch: 15 [448/3244 (14%)]\tMSE Loss: 0.036522\n",
      "Train Epoch: 15 [512/3244 (16%)]\tMSE Loss: 0.037688\n",
      "Train Epoch: 15 [576/3244 (18%)]\tMSE Loss: 0.037919\n",
      "Train Epoch: 15 [640/3244 (20%)]\tMSE Loss: 0.036750\n",
      "Train Epoch: 15 [704/3244 (22%)]\tMSE Loss: 0.037286\n",
      "Train Epoch: 15 [768/3244 (24%)]\tMSE Loss: 0.038646\n",
      "Train Epoch: 15 [832/3244 (25%)]\tMSE Loss: 0.037592\n",
      "Train Epoch: 15 [896/3244 (27%)]\tMSE Loss: 0.036921\n",
      "Train Epoch: 15 [960/3244 (29%)]\tMSE Loss: 0.035662\n",
      "Train Epoch: 15 [1024/3244 (31%)]\tMSE Loss: 0.035384\n",
      "Train Epoch: 15 [1088/3244 (33%)]\tMSE Loss: 0.035678\n",
      "Train Epoch: 15 [1152/3244 (35%)]\tMSE Loss: 0.037135\n",
      "Train Epoch: 15 [1216/3244 (37%)]\tMSE Loss: 0.036988\n",
      "Train Epoch: 15 [1280/3244 (39%)]\tMSE Loss: 0.036474\n",
      "Train Epoch: 15 [1344/3244 (41%)]\tMSE Loss: 0.039747\n",
      "Train Epoch: 15 [1408/3244 (43%)]\tMSE Loss: 0.040405\n",
      "Train Epoch: 15 [1472/3244 (45%)]\tMSE Loss: 0.037340\n",
      "Train Epoch: 15 [1536/3244 (47%)]\tMSE Loss: 0.036301\n",
      "Train Epoch: 15 [1600/3244 (49%)]\tMSE Loss: 0.040917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [1664/3244 (51%)]\tMSE Loss: 0.036431\n",
      "Train Epoch: 15 [1728/3244 (53%)]\tMSE Loss: 0.036940\n",
      "Train Epoch: 15 [1792/3244 (55%)]\tMSE Loss: 0.038979\n",
      "Train Epoch: 15 [1856/3244 (57%)]\tMSE Loss: 0.037723\n",
      "Train Epoch: 15 [1920/3244 (59%)]\tMSE Loss: 0.037069\n",
      "Train Epoch: 15 [1984/3244 (61%)]\tMSE Loss: 0.037117\n",
      "Train Epoch: 15 [2048/3244 (63%)]\tMSE Loss: 0.037140\n",
      "Train Epoch: 15 [2112/3244 (65%)]\tMSE Loss: 0.036625\n",
      "Train Epoch: 15 [2176/3244 (67%)]\tMSE Loss: 0.036072\n",
      "Train Epoch: 15 [2240/3244 (69%)]\tMSE Loss: 0.037091\n",
      "Train Epoch: 15 [2304/3244 (71%)]\tMSE Loss: 0.038008\n",
      "Train Epoch: 15 [2368/3244 (73%)]\tMSE Loss: 0.038072\n",
      "Train Epoch: 15 [2432/3244 (75%)]\tMSE Loss: 0.035179\n",
      "Train Epoch: 15 [2496/3244 (76%)]\tMSE Loss: 0.036248\n",
      "Train Epoch: 15 [2560/3244 (78%)]\tMSE Loss: 0.035634\n",
      "Train Epoch: 15 [2624/3244 (80%)]\tMSE Loss: 0.037795\n",
      "Train Epoch: 15 [2688/3244 (82%)]\tMSE Loss: 0.035877\n",
      "Train Epoch: 15 [2752/3244 (84%)]\tMSE Loss: 0.038047\n",
      "Train Epoch: 15 [2816/3244 (86%)]\tMSE Loss: 0.037311\n",
      "Train Epoch: 15 [2880/3244 (88%)]\tMSE Loss: 0.036789\n",
      "Train Epoch: 15 [2944/3244 (90%)]\tMSE Loss: 0.037736\n",
      "Train Epoch: 15 [3008/3244 (92%)]\tMSE Loss: 0.035081\n",
      "Train Epoch: 15 [3072/3244 (94%)]\tMSE Loss: 0.036148\n",
      "Train Epoch: 15 [3136/3244 (96%)]\tMSE Loss: 0.038108\n",
      "Train Epoch: 15 [2200/3244 (98%)]\tMSE Loss: 0.039905\n",
      "Train Epoch: 16 [0/3244 (0%)]\tMSE Loss: 0.040637\n",
      "Train Epoch: 16 [64/3244 (2%)]\tMSE Loss: 0.037633\n",
      "Train Epoch: 16 [128/3244 (4%)]\tMSE Loss: 0.037797\n",
      "Train Epoch: 16 [192/3244 (6%)]\tMSE Loss: 0.038657\n",
      "Train Epoch: 16 [256/3244 (8%)]\tMSE Loss: 0.036342\n",
      "Train Epoch: 16 [320/3244 (10%)]\tMSE Loss: 0.038041\n",
      "Train Epoch: 16 [384/3244 (12%)]\tMSE Loss: 0.035501\n",
      "Train Epoch: 16 [448/3244 (14%)]\tMSE Loss: 0.037200\n",
      "Train Epoch: 16 [512/3244 (16%)]\tMSE Loss: 0.037046\n",
      "Train Epoch: 16 [576/3244 (18%)]\tMSE Loss: 0.037099\n",
      "Train Epoch: 16 [640/3244 (20%)]\tMSE Loss: 0.036085\n",
      "Train Epoch: 16 [704/3244 (22%)]\tMSE Loss: 0.037851\n",
      "Train Epoch: 16 [768/3244 (24%)]\tMSE Loss: 0.037247\n",
      "Train Epoch: 16 [832/3244 (25%)]\tMSE Loss: 0.036369\n",
      "Train Epoch: 16 [896/3244 (27%)]\tMSE Loss: 0.035818\n",
      "Train Epoch: 16 [960/3244 (29%)]\tMSE Loss: 0.034718\n",
      "Train Epoch: 16 [1024/3244 (31%)]\tMSE Loss: 0.036778\n",
      "Train Epoch: 16 [1088/3244 (33%)]\tMSE Loss: 0.036600\n",
      "Train Epoch: 16 [1152/3244 (35%)]\tMSE Loss: 0.035700\n",
      "Train Epoch: 16 [1216/3244 (37%)]\tMSE Loss: 0.035306\n",
      "Train Epoch: 16 [1280/3244 (39%)]\tMSE Loss: 0.035241\n",
      "Train Epoch: 16 [1344/3244 (41%)]\tMSE Loss: 0.038280\n",
      "Train Epoch: 16 [1408/3244 (43%)]\tMSE Loss: 0.036140\n",
      "Train Epoch: 16 [1472/3244 (45%)]\tMSE Loss: 0.034823\n",
      "Train Epoch: 16 [1536/3244 (47%)]\tMSE Loss: 0.035529\n",
      "Train Epoch: 16 [1600/3244 (49%)]\tMSE Loss: 0.040184\n",
      "Train Epoch: 16 [1664/3244 (51%)]\tMSE Loss: 0.038845\n",
      "Train Epoch: 16 [1728/3244 (53%)]\tMSE Loss: 0.035073\n",
      "Train Epoch: 16 [1792/3244 (55%)]\tMSE Loss: 0.035250\n",
      "Train Epoch: 16 [1856/3244 (57%)]\tMSE Loss: 0.040435\n",
      "Train Epoch: 16 [1920/3244 (59%)]\tMSE Loss: 0.043180\n",
      "Train Epoch: 16 [1984/3244 (61%)]\tMSE Loss: 0.037473\n",
      "Train Epoch: 16 [2048/3244 (63%)]\tMSE Loss: 0.038434\n",
      "Train Epoch: 16 [2112/3244 (65%)]\tMSE Loss: 0.037911\n",
      "Train Epoch: 16 [2176/3244 (67%)]\tMSE Loss: 0.039248\n",
      "Train Epoch: 16 [2240/3244 (69%)]\tMSE Loss: 0.036076\n",
      "Train Epoch: 16 [2304/3244 (71%)]\tMSE Loss: 0.038383\n",
      "Train Epoch: 16 [2368/3244 (73%)]\tMSE Loss: 0.040866\n",
      "Train Epoch: 16 [2432/3244 (75%)]\tMSE Loss: 0.040646\n",
      "Train Epoch: 16 [2496/3244 (76%)]\tMSE Loss: 0.039958\n",
      "Train Epoch: 16 [2560/3244 (78%)]\tMSE Loss: 0.037893\n",
      "Train Epoch: 16 [2624/3244 (80%)]\tMSE Loss: 0.038556\n",
      "Train Epoch: 16 [2688/3244 (82%)]\tMSE Loss: 0.038128\n",
      "Train Epoch: 16 [2752/3244 (84%)]\tMSE Loss: 0.034956\n",
      "Train Epoch: 16 [2816/3244 (86%)]\tMSE Loss: 0.037430\n",
      "Train Epoch: 16 [2880/3244 (88%)]\tMSE Loss: 0.038107\n",
      "Train Epoch: 16 [2944/3244 (90%)]\tMSE Loss: 0.034996\n",
      "Train Epoch: 16 [3008/3244 (92%)]\tMSE Loss: 0.036977\n",
      "Train Epoch: 16 [3072/3244 (94%)]\tMSE Loss: 0.036986\n",
      "Train Epoch: 16 [3136/3244 (96%)]\tMSE Loss: 0.037573\n",
      "Train Epoch: 16 [2200/3244 (98%)]\tMSE Loss: 0.036918\n",
      "Train Epoch: 17 [0/3244 (0%)]\tMSE Loss: 0.035547\n",
      "Train Epoch: 17 [64/3244 (2%)]\tMSE Loss: 0.037068\n",
      "Train Epoch: 17 [128/3244 (4%)]\tMSE Loss: 0.035092\n",
      "Train Epoch: 17 [192/3244 (6%)]\tMSE Loss: 0.036597\n",
      "Train Epoch: 17 [256/3244 (8%)]\tMSE Loss: 0.036377\n",
      "Train Epoch: 17 [320/3244 (10%)]\tMSE Loss: 0.035111\n",
      "Train Epoch: 17 [384/3244 (12%)]\tMSE Loss: 0.034992\n",
      "Train Epoch: 17 [448/3244 (14%)]\tMSE Loss: 0.034996\n",
      "Train Epoch: 17 [512/3244 (16%)]\tMSE Loss: 0.037581\n",
      "Train Epoch: 17 [576/3244 (18%)]\tMSE Loss: 0.035631\n",
      "Train Epoch: 17 [640/3244 (20%)]\tMSE Loss: 0.037406\n",
      "Train Epoch: 17 [704/3244 (22%)]\tMSE Loss: 0.034293\n",
      "Train Epoch: 17 [768/3244 (24%)]\tMSE Loss: 0.037319\n",
      "Train Epoch: 17 [832/3244 (25%)]\tMSE Loss: 0.037874\n",
      "Train Epoch: 17 [896/3244 (27%)]\tMSE Loss: 0.037467\n",
      "Train Epoch: 17 [960/3244 (29%)]\tMSE Loss: 0.035257\n",
      "Train Epoch: 17 [1024/3244 (31%)]\tMSE Loss: 0.036447\n",
      "Train Epoch: 17 [1088/3244 (33%)]\tMSE Loss: 0.036995\n",
      "Train Epoch: 17 [1152/3244 (35%)]\tMSE Loss: 0.037254\n",
      "Train Epoch: 17 [1216/3244 (37%)]\tMSE Loss: 0.037616\n",
      "Train Epoch: 17 [1280/3244 (39%)]\tMSE Loss: 0.036447\n",
      "Train Epoch: 17 [1344/3244 (41%)]\tMSE Loss: 0.038854\n",
      "Train Epoch: 17 [1408/3244 (43%)]\tMSE Loss: 0.035920\n",
      "Train Epoch: 17 [1472/3244 (45%)]\tMSE Loss: 0.033553\n",
      "Train Epoch: 17 [1536/3244 (47%)]\tMSE Loss: 0.036116\n",
      "Train Epoch: 17 [1600/3244 (49%)]\tMSE Loss: 0.036084\n",
      "Train Epoch: 17 [1664/3244 (51%)]\tMSE Loss: 0.036017\n",
      "Train Epoch: 17 [1728/3244 (53%)]\tMSE Loss: 0.040365\n",
      "Train Epoch: 17 [1792/3244 (55%)]\tMSE Loss: 0.037571\n",
      "Train Epoch: 17 [1856/3244 (57%)]\tMSE Loss: 0.036605\n",
      "Train Epoch: 17 [1920/3244 (59%)]\tMSE Loss: 0.035242\n",
      "Train Epoch: 17 [1984/3244 (61%)]\tMSE Loss: 0.034770\n",
      "Train Epoch: 17 [2048/3244 (63%)]\tMSE Loss: 0.036848\n",
      "Train Epoch: 17 [2112/3244 (65%)]\tMSE Loss: 0.035608\n",
      "Train Epoch: 17 [2176/3244 (67%)]\tMSE Loss: 0.039671\n",
      "Train Epoch: 17 [2240/3244 (69%)]\tMSE Loss: 0.036700\n",
      "Train Epoch: 17 [2304/3244 (71%)]\tMSE Loss: 0.036149\n",
      "Train Epoch: 17 [2368/3244 (73%)]\tMSE Loss: 0.035295\n",
      "Train Epoch: 17 [2432/3244 (75%)]\tMSE Loss: 0.036663\n",
      "Train Epoch: 17 [2496/3244 (76%)]\tMSE Loss: 0.037323\n",
      "Train Epoch: 17 [2560/3244 (78%)]\tMSE Loss: 0.036302\n",
      "Train Epoch: 17 [2624/3244 (80%)]\tMSE Loss: 0.035652\n",
      "Train Epoch: 17 [2688/3244 (82%)]\tMSE Loss: 0.035930\n",
      "Train Epoch: 17 [2752/3244 (84%)]\tMSE Loss: 0.036314\n",
      "Train Epoch: 17 [2816/3244 (86%)]\tMSE Loss: 0.035357\n",
      "Train Epoch: 17 [2880/3244 (88%)]\tMSE Loss: 0.034845\n",
      "Train Epoch: 17 [2944/3244 (90%)]\tMSE Loss: 0.035964\n",
      "Train Epoch: 17 [3008/3244 (92%)]\tMSE Loss: 0.034142\n",
      "Train Epoch: 17 [3072/3244 (94%)]\tMSE Loss: 0.038346\n",
      "Train Epoch: 17 [3136/3244 (96%)]\tMSE Loss: 0.037955\n",
      "Train Epoch: 17 [2200/3244 (98%)]\tMSE Loss: 0.035409\n",
      "Train Epoch: 18 [0/3244 (0%)]\tMSE Loss: 0.035166\n",
      "Train Epoch: 18 [64/3244 (2%)]\tMSE Loss: 0.036027\n",
      "Train Epoch: 18 [128/3244 (4%)]\tMSE Loss: 0.036078\n",
      "Train Epoch: 18 [192/3244 (6%)]\tMSE Loss: 0.035231\n",
      "Train Epoch: 18 [256/3244 (8%)]\tMSE Loss: 0.035034\n",
      "Train Epoch: 18 [320/3244 (10%)]\tMSE Loss: 0.036483\n",
      "Train Epoch: 18 [384/3244 (12%)]\tMSE Loss: 0.033903\n",
      "Train Epoch: 18 [448/3244 (14%)]\tMSE Loss: 0.035623\n",
      "Train Epoch: 18 [512/3244 (16%)]\tMSE Loss: 0.036091\n",
      "Train Epoch: 18 [576/3244 (18%)]\tMSE Loss: 0.035782\n",
      "Train Epoch: 18 [640/3244 (20%)]\tMSE Loss: 0.035634\n",
      "Train Epoch: 18 [704/3244 (22%)]\tMSE Loss: 0.035901\n",
      "Train Epoch: 18 [768/3244 (24%)]\tMSE Loss: 0.035815\n",
      "Train Epoch: 18 [832/3244 (25%)]\tMSE Loss: 0.035061\n",
      "Train Epoch: 18 [896/3244 (27%)]\tMSE Loss: 0.037305\n",
      "Train Epoch: 18 [960/3244 (29%)]\tMSE Loss: 0.037545\n",
      "Train Epoch: 18 [1024/3244 (31%)]\tMSE Loss: 0.036692\n",
      "Train Epoch: 18 [1088/3244 (33%)]\tMSE Loss: 0.037598\n",
      "Train Epoch: 18 [1152/3244 (35%)]\tMSE Loss: 0.034326\n",
      "Train Epoch: 18 [1216/3244 (37%)]\tMSE Loss: 0.034607\n",
      "Train Epoch: 18 [1280/3244 (39%)]\tMSE Loss: 0.035604\n",
      "Train Epoch: 18 [1344/3244 (41%)]\tMSE Loss: 0.037734\n",
      "Train Epoch: 18 [1408/3244 (43%)]\tMSE Loss: 0.035777\n",
      "Train Epoch: 18 [1472/3244 (45%)]\tMSE Loss: 0.034862\n",
      "Train Epoch: 18 [1536/3244 (47%)]\tMSE Loss: 0.035947\n",
      "Train Epoch: 18 [1600/3244 (49%)]\tMSE Loss: 0.037043\n",
      "Train Epoch: 18 [1664/3244 (51%)]\tMSE Loss: 0.037070\n",
      "Train Epoch: 18 [1728/3244 (53%)]\tMSE Loss: 0.035141\n",
      "Train Epoch: 18 [1792/3244 (55%)]\tMSE Loss: 0.034687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 18 [1856/3244 (57%)]\tMSE Loss: 0.037935\n",
      "Train Epoch: 18 [1920/3244 (59%)]\tMSE Loss: 0.039564\n",
      "Train Epoch: 18 [1984/3244 (61%)]\tMSE Loss: 0.040400\n",
      "Train Epoch: 18 [2048/3244 (63%)]\tMSE Loss: 0.038066\n",
      "Train Epoch: 18 [2112/3244 (65%)]\tMSE Loss: 0.036149\n",
      "Train Epoch: 18 [2176/3244 (67%)]\tMSE Loss: 0.038641\n",
      "Train Epoch: 18 [2240/3244 (69%)]\tMSE Loss: 0.039512\n",
      "Train Epoch: 18 [2304/3244 (71%)]\tMSE Loss: 0.035540\n",
      "Train Epoch: 18 [2368/3244 (73%)]\tMSE Loss: 0.036923\n",
      "Train Epoch: 18 [2432/3244 (75%)]\tMSE Loss: 0.035672\n",
      "Train Epoch: 18 [2496/3244 (76%)]\tMSE Loss: 0.038996\n",
      "Train Epoch: 18 [2560/3244 (78%)]\tMSE Loss: 0.036135\n",
      "Train Epoch: 18 [2624/3244 (80%)]\tMSE Loss: 0.036482\n",
      "Train Epoch: 18 [2688/3244 (82%)]\tMSE Loss: 0.035615\n",
      "Train Epoch: 18 [2752/3244 (84%)]\tMSE Loss: 0.033724\n",
      "Train Epoch: 18 [2816/3244 (86%)]\tMSE Loss: 0.036338\n",
      "Train Epoch: 18 [2880/3244 (88%)]\tMSE Loss: 0.036383\n",
      "Train Epoch: 18 [2944/3244 (90%)]\tMSE Loss: 0.034600\n",
      "Train Epoch: 18 [3008/3244 (92%)]\tMSE Loss: 0.034466\n",
      "Train Epoch: 18 [3072/3244 (94%)]\tMSE Loss: 0.037267\n",
      "Train Epoch: 18 [3136/3244 (96%)]\tMSE Loss: 0.037833\n",
      "Train Epoch: 18 [2200/3244 (98%)]\tMSE Loss: 0.036215\n",
      "Train Epoch: 19 [0/3244 (0%)]\tMSE Loss: 0.035660\n",
      "Train Epoch: 19 [64/3244 (2%)]\tMSE Loss: 0.036309\n",
      "Train Epoch: 19 [128/3244 (4%)]\tMSE Loss: 0.035264\n",
      "Train Epoch: 19 [192/3244 (6%)]\tMSE Loss: 0.035741\n",
      "Train Epoch: 19 [256/3244 (8%)]\tMSE Loss: 0.034017\n",
      "Train Epoch: 19 [320/3244 (10%)]\tMSE Loss: 0.036431\n",
      "Train Epoch: 19 [384/3244 (12%)]\tMSE Loss: 0.035336\n",
      "Train Epoch: 19 [448/3244 (14%)]\tMSE Loss: 0.035931\n",
      "Train Epoch: 19 [512/3244 (16%)]\tMSE Loss: 0.035492\n",
      "Train Epoch: 19 [576/3244 (18%)]\tMSE Loss: 0.036416\n",
      "Train Epoch: 19 [640/3244 (20%)]\tMSE Loss: 0.036925\n",
      "Train Epoch: 19 [704/3244 (22%)]\tMSE Loss: 0.036331\n",
      "Train Epoch: 19 [768/3244 (24%)]\tMSE Loss: 0.035717\n",
      "Train Epoch: 19 [832/3244 (25%)]\tMSE Loss: 0.035170\n",
      "Train Epoch: 19 [896/3244 (27%)]\tMSE Loss: 0.036545\n",
      "Train Epoch: 19 [960/3244 (29%)]\tMSE Loss: 0.034506\n",
      "Train Epoch: 19 [1024/3244 (31%)]\tMSE Loss: 0.037269\n",
      "Train Epoch: 19 [1088/3244 (33%)]\tMSE Loss: 0.034961\n",
      "Train Epoch: 19 [1152/3244 (35%)]\tMSE Loss: 0.033580\n",
      "Train Epoch: 19 [1216/3244 (37%)]\tMSE Loss: 0.034958\n",
      "Train Epoch: 19 [1280/3244 (39%)]\tMSE Loss: 0.034956\n",
      "Train Epoch: 19 [1344/3244 (41%)]\tMSE Loss: 0.036936\n",
      "Train Epoch: 19 [1408/3244 (43%)]\tMSE Loss: 0.036257\n",
      "Train Epoch: 19 [1472/3244 (45%)]\tMSE Loss: 0.035049\n",
      "Train Epoch: 19 [1536/3244 (47%)]\tMSE Loss: 0.037413\n",
      "Train Epoch: 19 [1600/3244 (49%)]\tMSE Loss: 0.035850\n",
      "Train Epoch: 19 [1664/3244 (51%)]\tMSE Loss: 0.036863\n",
      "Train Epoch: 19 [1728/3244 (53%)]\tMSE Loss: 0.035908\n",
      "Train Epoch: 19 [1792/3244 (55%)]\tMSE Loss: 0.033921\n",
      "Train Epoch: 19 [1856/3244 (57%)]\tMSE Loss: 0.037115\n",
      "Train Epoch: 19 [1920/3244 (59%)]\tMSE Loss: 0.035845\n",
      "Train Epoch: 19 [1984/3244 (61%)]\tMSE Loss: 0.036308\n",
      "Train Epoch: 19 [2048/3244 (63%)]\tMSE Loss: 0.036311\n",
      "Train Epoch: 19 [2112/3244 (65%)]\tMSE Loss: 0.035354\n",
      "Train Epoch: 19 [2176/3244 (67%)]\tMSE Loss: 0.033783\n",
      "Train Epoch: 19 [2240/3244 (69%)]\tMSE Loss: 0.034384\n",
      "Train Epoch: 19 [2304/3244 (71%)]\tMSE Loss: 0.036210\n",
      "Train Epoch: 19 [2368/3244 (73%)]\tMSE Loss: 0.036642\n",
      "Train Epoch: 19 [2432/3244 (75%)]\tMSE Loss: 0.033935\n",
      "Train Epoch: 19 [2496/3244 (76%)]\tMSE Loss: 0.034946\n",
      "Train Epoch: 19 [2560/3244 (78%)]\tMSE Loss: 0.037554\n",
      "Train Epoch: 19 [2624/3244 (80%)]\tMSE Loss: 0.036641\n",
      "Train Epoch: 19 [2688/3244 (82%)]\tMSE Loss: 0.036056\n",
      "Train Epoch: 19 [2752/3244 (84%)]\tMSE Loss: 0.035401\n",
      "Train Epoch: 19 [2816/3244 (86%)]\tMSE Loss: 0.034458\n",
      "Train Epoch: 19 [2880/3244 (88%)]\tMSE Loss: 0.034509\n",
      "Train Epoch: 19 [2944/3244 (90%)]\tMSE Loss: 0.035813\n",
      "Train Epoch: 19 [3008/3244 (92%)]\tMSE Loss: 0.035123\n",
      "Train Epoch: 19 [3072/3244 (94%)]\tMSE Loss: 0.037787\n",
      "Train Epoch: 19 [3136/3244 (96%)]\tMSE Loss: 0.034853\n",
      "Train Epoch: 19 [2200/3244 (98%)]\tMSE Loss: 0.032672\n",
      "Train Epoch: 20 [0/3244 (0%)]\tMSE Loss: 0.033775\n",
      "Train Epoch: 20 [64/3244 (2%)]\tMSE Loss: 0.036360\n",
      "Train Epoch: 20 [128/3244 (4%)]\tMSE Loss: 0.034782\n",
      "Train Epoch: 20 [192/3244 (6%)]\tMSE Loss: 0.036087\n",
      "Train Epoch: 20 [256/3244 (8%)]\tMSE Loss: 0.036569\n",
      "Train Epoch: 20 [320/3244 (10%)]\tMSE Loss: 0.034846\n",
      "Train Epoch: 20 [384/3244 (12%)]\tMSE Loss: 0.037139\n",
      "Train Epoch: 20 [448/3244 (14%)]\tMSE Loss: 0.036921\n",
      "Train Epoch: 20 [512/3244 (16%)]\tMSE Loss: 0.035992\n",
      "Train Epoch: 20 [576/3244 (18%)]\tMSE Loss: 0.036421\n",
      "Train Epoch: 20 [640/3244 (20%)]\tMSE Loss: 0.035830\n",
      "Train Epoch: 20 [704/3244 (22%)]\tMSE Loss: 0.034616\n",
      "Train Epoch: 20 [768/3244 (24%)]\tMSE Loss: 0.033907\n",
      "Train Epoch: 20 [832/3244 (25%)]\tMSE Loss: 0.034593\n",
      "Train Epoch: 20 [896/3244 (27%)]\tMSE Loss: 0.036754\n",
      "Train Epoch: 20 [960/3244 (29%)]\tMSE Loss: 0.034550\n",
      "Train Epoch: 20 [1024/3244 (31%)]\tMSE Loss: 0.035848\n",
      "Train Epoch: 20 [1088/3244 (33%)]\tMSE Loss: 0.036014\n",
      "Train Epoch: 20 [1152/3244 (35%)]\tMSE Loss: 0.034706\n",
      "Train Epoch: 20 [1216/3244 (37%)]\tMSE Loss: 0.036112\n",
      "Train Epoch: 20 [1280/3244 (39%)]\tMSE Loss: 0.036731\n",
      "Train Epoch: 20 [1344/3244 (41%)]\tMSE Loss: 0.033948\n",
      "Train Epoch: 20 [1408/3244 (43%)]\tMSE Loss: 0.034743\n",
      "Train Epoch: 20 [1472/3244 (45%)]\tMSE Loss: 0.034617\n",
      "Train Epoch: 20 [1536/3244 (47%)]\tMSE Loss: 0.035264\n",
      "Train Epoch: 20 [1600/3244 (49%)]\tMSE Loss: 0.032379\n",
      "Train Epoch: 20 [1664/3244 (51%)]\tMSE Loss: 0.034300\n",
      "Train Epoch: 20 [1728/3244 (53%)]\tMSE Loss: 0.036988\n",
      "Train Epoch: 20 [1792/3244 (55%)]\tMSE Loss: 0.035123\n",
      "Train Epoch: 20 [1856/3244 (57%)]\tMSE Loss: 0.036893\n",
      "Train Epoch: 20 [1920/3244 (59%)]\tMSE Loss: 0.035590\n",
      "Train Epoch: 20 [1984/3244 (61%)]\tMSE Loss: 0.036341\n",
      "Train Epoch: 20 [2048/3244 (63%)]\tMSE Loss: 0.037889\n",
      "Train Epoch: 20 [2112/3244 (65%)]\tMSE Loss: 0.034313\n",
      "Train Epoch: 20 [2176/3244 (67%)]\tMSE Loss: 0.034077\n",
      "Train Epoch: 20 [2240/3244 (69%)]\tMSE Loss: 0.034291\n",
      "Train Epoch: 20 [2304/3244 (71%)]\tMSE Loss: 0.033506\n",
      "Train Epoch: 20 [2368/3244 (73%)]\tMSE Loss: 0.036815\n",
      "Train Epoch: 20 [2432/3244 (75%)]\tMSE Loss: 0.032764\n",
      "Train Epoch: 20 [2496/3244 (76%)]\tMSE Loss: 0.034072\n",
      "Train Epoch: 20 [2560/3244 (78%)]\tMSE Loss: 0.035942\n",
      "Train Epoch: 20 [2624/3244 (80%)]\tMSE Loss: 0.035846\n",
      "Train Epoch: 20 [2688/3244 (82%)]\tMSE Loss: 0.034516\n",
      "Train Epoch: 20 [2752/3244 (84%)]\tMSE Loss: 0.035802\n",
      "Train Epoch: 20 [2816/3244 (86%)]\tMSE Loss: 0.034015\n",
      "Train Epoch: 20 [2880/3244 (88%)]\tMSE Loss: 0.034513\n",
      "Train Epoch: 20 [2944/3244 (90%)]\tMSE Loss: 0.034903\n",
      "Train Epoch: 20 [3008/3244 (92%)]\tMSE Loss: 0.034326\n",
      "Train Epoch: 20 [3072/3244 (94%)]\tMSE Loss: 0.035068\n",
      "Train Epoch: 20 [3136/3244 (96%)]\tMSE Loss: 0.034331\n",
      "Train Epoch: 20 [2200/3244 (98%)]\tMSE Loss: 0.033677\n",
      "Train Epoch: 21 [0/3244 (0%)]\tMSE Loss: 0.034267\n",
      "Train Epoch: 21 [64/3244 (2%)]\tMSE Loss: 0.038665\n",
      "Train Epoch: 21 [128/3244 (4%)]\tMSE Loss: 0.039334\n",
      "Train Epoch: 21 [192/3244 (6%)]\tMSE Loss: 0.037419\n",
      "Train Epoch: 21 [256/3244 (8%)]\tMSE Loss: 0.033810\n",
      "Train Epoch: 21 [320/3244 (10%)]\tMSE Loss: 0.034983\n",
      "Train Epoch: 21 [384/3244 (12%)]\tMSE Loss: 0.035598\n",
      "Train Epoch: 21 [448/3244 (14%)]\tMSE Loss: 0.034339\n",
      "Train Epoch: 21 [512/3244 (16%)]\tMSE Loss: 0.035333\n",
      "Train Epoch: 21 [576/3244 (18%)]\tMSE Loss: 0.034339\n",
      "Train Epoch: 21 [640/3244 (20%)]\tMSE Loss: 0.036041\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-95f4e2f703ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-88a01f1b8681>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "input_size = 160 * 160\n",
    "output_size = 160 * 160\n",
    "\n",
    "model_cnn = CNN(input_size, output_size)\n",
    "model_cnn.float().to(device)\n",
    "optimizer = optim.Adam(model_cnn.parameters(), lr = 0.0001)\n",
    "\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(n_epochs): train(epoch, model_cnn, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "test(model_cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110ec9ac8>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
